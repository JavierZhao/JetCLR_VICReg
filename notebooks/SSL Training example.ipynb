{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9dd30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:19.941042Z",
     "start_time": "2023-10-31T18:14:17.950902Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1b4e9",
   "metadata": {},
   "source": [
    "## Vanilla Transformer Encoder for the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f27a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:19.971523Z",
     "start_time": "2023-10-31T18:14:19.944371Z"
    }
   },
   "outputs": [],
   "source": [
    "# class for transformer network\n",
    "class Transformer(nn.Module):\n",
    "    # define and intialize the structure of the neural network\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=3,\n",
    "        model_dim=32,\n",
    "        output_dim=32,\n",
    "        n_heads=4,\n",
    "        dim_feedforward=32,\n",
    "        n_layers=4,\n",
    "        learning_rate=0.00005,\n",
    "        n_head_layers=2,\n",
    "        head_norm=False,\n",
    "        dropout=0.1,\n",
    "        opt=\"adam\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # define hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_head_layers = n_head_layers\n",
    "        self.head_norm = head_norm\n",
    "        self.dropout = dropout\n",
    "        # define subnetworks\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                model_dim, n_heads, dim_feedforward=dim_feedforward, dropout=dropout\n",
    "            ),\n",
    "            n_layers,\n",
    "        )\n",
    "        # head_layers have output_dim\n",
    "        if n_head_layers == 0:\n",
    "            self.head_layers = []\n",
    "        else:\n",
    "            if head_norm:\n",
    "                self.norm_layers = nn.ModuleList([nn.LayerNorm(model_dim)])\n",
    "            self.head_layers = nn.ModuleList([nn.Linear(model_dim, output_dim)])\n",
    "            for i in range(n_head_layers - 1):\n",
    "                if head_norm:\n",
    "                    self.norm_layers.append(nn.LayerNorm(output_dim))\n",
    "                self.head_layers.append(nn.Linear(output_dim, output_dim))\n",
    "        # option to use adam or sgd\n",
    "        if opt == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        if opt == \"sgdca\" or opt == \"sgdslr\" or opt == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(\n",
    "                self.parameters(), lr=self.learning_rate, momentum=0.9\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inpt,\n",
    "        mask=None,\n",
    "        use_mask=False,\n",
    "        use_continuous_mask=False,\n",
    "        mult_reps=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input here is (batch_size, n_constit, 3)\n",
    "        but transformer expects (n_constit, batch_size, 3) so we need to transpose\n",
    "        if use_mask is True, will mask out all inputs with pT=0\n",
    "        \"\"\"\n",
    "        assert not (use_mask and use_continuous_mask)\n",
    "        # make a copy\n",
    "        x = inpt + 0.0\n",
    "        # (batch_size, n_constit)\n",
    "        if use_mask:\n",
    "            pT_zero = x[:, :, 0] == 0\n",
    "        # (batch_size, n_constit)\n",
    "        if use_continuous_mask:\n",
    "            pT = x[:, :, 0]\n",
    "        if use_mask:\n",
    "            mask = self.make_mask(pT_zero).to(x.device)\n",
    "        elif use_continuous_mask:\n",
    "            mask = self.make_continuous_mask(pT).to(x.device)\n",
    "        else:\n",
    "            mask = None\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        # (n_constit, batch_size, model_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        if use_mask:\n",
    "            # set masked constituents to zero\n",
    "            # otherwise the sum will change if the constituents with 0 pT change\n",
    "            x[torch.transpose(pT_zero, 0, 1)] = 0\n",
    "        elif use_continuous_mask:\n",
    "            # scale x by pT, so that function is IR safe\n",
    "            # transpose first to get correct shape\n",
    "            x *= torch.transpose(pT, 0, 1)[:, :, None]\n",
    "        # sum over sequence dim\n",
    "        # (batch_size, model_dim)\n",
    "        x = x.sum(0)\n",
    "        return self.head(x, mult_reps)\n",
    "\n",
    "    def head(self, x, mult_reps):\n",
    "        \"\"\"\n",
    "        calculates output of the head if it exists, i.e. if n_head_layer>0\n",
    "        returns multiple representation layers if asked for by mult_reps = True\n",
    "        input:  x shape=(batchsize, model_dim)\n",
    "                mult_reps boolean\n",
    "        output: reps shape=(batchsize, output_dim)                  for mult_reps=False\n",
    "                reps shape=(batchsize, number_of_reps, output_dim)  for mult_reps=True\n",
    "        \"\"\"\n",
    "        relu = nn.ReLU()\n",
    "        # return representations from multiple layers for evaluation\n",
    "        if mult_reps == True:\n",
    "            if self.n_head_layers > 0:\n",
    "                reps = torch.empty(x.shape[0], self.n_head_layers + 1, self.output_dim)\n",
    "                reps[:, 0] = x\n",
    "                for i, layer in enumerate(self.head_layers):\n",
    "                    # only apply layer norm on head if chosen\n",
    "                    if self.head_norm:\n",
    "                        x = self.norm_layers[i](x)\n",
    "                    x = relu(x)\n",
    "                    x = layer(x)\n",
    "                    reps[:, i + 1] = x\n",
    "                # shape (n_head_layers, output_dim)\n",
    "                return reps\n",
    "            # no head exists -> just return x in a list with dimension 1\n",
    "            else:\n",
    "                reps = x[:, None, :]\n",
    "                # shape (batchsize, 1, model_dim)\n",
    "                return reps\n",
    "        # return only last representation for contrastive loss\n",
    "        else:\n",
    "            for i, layer in enumerate(\n",
    "                self.head_layers\n",
    "            ):  # will do nothing if n_head_layers is 0\n",
    "                if self.head_norm:\n",
    "                    x = self.norm_layers[i](x)\n",
    "                x = relu(x)\n",
    "                x = layer(x)\n",
    "            # shape either (model_dim) if no head, or (output_dim) if head exists\n",
    "            return x\n",
    "\n",
    "    def forward_batchwise(\n",
    "        self, x, batch_size, use_mask=False, use_continuous_mask=False\n",
    "    ):\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            if self.n_head_layers == 0:\n",
    "                rep_dim = self.model_dim\n",
    "                number_of_reps = 1\n",
    "            elif self.n_head_layers > 0:\n",
    "                rep_dim = self.output_dim\n",
    "                number_of_reps = self.n_head_layers + 1\n",
    "            out = torch.empty(x.size(0), number_of_reps, rep_dim)\n",
    "            idx_list = torch.split(torch.arange(x.size(0)), batch_size)\n",
    "            for idx in idx_list:\n",
    "                output = (\n",
    "                    self(\n",
    "                        x[idx].to(device),\n",
    "                        use_mask=use_mask,\n",
    "                        use_continuous_mask=use_continuous_mask,\n",
    "                        mult_reps=True,\n",
    "                    )\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                )\n",
    "                out[idx] = output\n",
    "        return out\n",
    "\n",
    "    def make_mask(self, pT_zero):\n",
    "        \"\"\"\n",
    "        Input: batch of bools of whether pT=0, shape (batchsize, n_constit)\n",
    "        Output: mask for transformer model which masks out constituents with pT=0, shape (batchsize*n_transformer_heads, n_constit, n_constit)\n",
    "        mask is added to attention output before softmax: 0 means value is unchanged, -inf means it will be masked\n",
    "        \"\"\"\n",
    "        n_constit = pT_zero.size(1)\n",
    "        pT_zero = torch.repeat_interleave(pT_zero, self.n_heads, axis=0)\n",
    "        pT_zero = torch.repeat_interleave(pT_zero[:, None], n_constit, axis=1)\n",
    "        mask = torch.zeros(pT_zero.size(0), n_constit, n_constit)\n",
    "        mask[pT_zero] = -np.inf\n",
    "        return mask\n",
    "\n",
    "    def make_continuous_mask(self, pT):\n",
    "        \"\"\"\n",
    "        Input: batch of pT values, shape (batchsize, n_constit)\n",
    "        Output: mask for transformer model: -1/pT, shape (batchsize*n_transformer_heads, n_constit, n_constit)\n",
    "        mask is added to attention output before softmax: 0 means value is unchanged, -inf means it will be masked\n",
    "        intermediate values mean it is partly masked\n",
    "        This function implements IR safety in the transformer\n",
    "        \"\"\"\n",
    "        n_constit = pT.size(1)\n",
    "        pT_reshape = torch.repeat_interleave(pT, self.n_heads, axis=0)\n",
    "        pT_reshape = torch.repeat_interleave(pT_reshape[:, None], n_constit, axis=1)\n",
    "        # mask = -1/pT_reshape\n",
    "        mask = 0.5 * torch.log(pT_reshape)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3b8d0",
   "metadata": {},
   "source": [
    "## Define data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4535df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.004225Z",
     "start_time": "2023-10-31T18:14:19.973836Z"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_jets(batch, device):\n",
    "    \"\"\"\n",
    "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
    "    dim 1 ordering: (pT, eta, phi)\n",
    "    Output: batch of jets rotated independently in eta-phi, same shape as input\n",
    "    \"\"\"\n",
    "    rot_angle = torch.rand(batch.shape[0]) * 2 * torch.tensor(np.pi)\n",
    "    c = torch.cos(rot_angle)\n",
    "    s = torch.sin(rot_angle)\n",
    "    o = torch.ones_like(rot_angle)\n",
    "    z = torch.zeros_like(rot_angle)\n",
    "    rot_matrix = (\n",
    "        torch.stack([o, z, z, z, c, -s, z, s, c], dim=1)\n",
    "        .reshape(-1, 3, 3)\n",
    "        .transpose(0, 2)\n",
    "    ).to(\n",
    "        device\n",
    "    )  # (batchsize, 3, 3)\n",
    "    return torch.einsum(\"ijk,lji->ilk\", batch, rot_matrix)\n",
    "\n",
    "def augmentation(args, x, device):\n",
    "    \"\"\"\n",
    "    Applies all the augmentations specified in the args\n",
    "    \"\"\"\n",
    "    # cropping to 50 particles is already done in data preprocessing\n",
    "    y = x.clone()\n",
    "    x = rotate_jets(x, device)\n",
    "    \n",
    "    x = x.transpose(1, 2)  # [batch_size, 3, n_constit] -> [batch_size, n_constit, 3]\n",
    "    y = y.transpose(1, 2)  # [batch_size, 3, n_constit] -> [batch_size, n_constit, 3]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6504ba",
   "metadata": {},
   "source": [
    "## The VICReg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffae762a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.024415Z",
     "start_time": "2023-10-31T18:14:20.008061Z"
    }
   },
   "outputs": [],
   "source": [
    "class VICReg(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.num_features = int(\n",
    "            args.mlp.split(\"-\")[-1]\n",
    "        )  # size of the last layer of the MLP projector\n",
    "        self.x_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.x_inputs),\n",
    "            nn.Linear(args.x_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.y_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.y_inputs),\n",
    "            nn.Linear(args.y_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.augmentation = args.augmentation\n",
    "        self.x_backbone = args.x_backbone\n",
    "        self.y_backbone = args.y_backbone\n",
    "        self.N_x = self.x_backbone.input_dim\n",
    "        self.N_y = self.y_backbone.input_dim\n",
    "        self.embedding = args.Do\n",
    "        self.return_embedding = args.return_embedding\n",
    "        self.return_representation = args.return_representation\n",
    "        self.x_projector = Projector(args.mlp, self.embedding)\n",
    "        self.y_projector = (\n",
    "            self.x_projector if args.shared else copy.deepcopy(self.x_projector)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x -> x_aug -> (x_xform) -> x_rep -> x_emb\n",
    "        y -> y_aug -> (y_xform) -> y_rep -> y_emb\n",
    "        _aug: augmented\n",
    "        _xform: transformed by linear layer (skipped because it destroys the zero padding)\n",
    "        _rep: backbone representation\n",
    "        _emb: projected embedding\n",
    "        \"\"\"\n",
    "        x_aug, y_aug = self.augmentation(\n",
    "            self.args, x, self.args.device\n",
    "        )  # [batch_size, n_constit, 3]\n",
    "\n",
    "        x_rep = self.x_backbone(\n",
    "            x_aug, use_mask=self.args.mask, use_continuous_mask=self.args.cmask\n",
    "        )  # [batch_size, output_dim]\n",
    "        y_rep = self.y_backbone(\n",
    "            y_aug, use_mask=self.args.mask, use_continuous_mask=self.args.cmask\n",
    "        )  # [batch_size, output_dim]\n",
    "        if self.return_representation:\n",
    "            return x_rep, y_rep\n",
    "\n",
    "        x_emb = self.x_projector(x_rep)  # [batch_size, embedding_size]\n",
    "        y_emb = self.y_projector(y_rep)  # [batch_size, embedding_size]\n",
    "        if self.return_embedding:\n",
    "            return x_emb, y_emb\n",
    "        x = x_emb\n",
    "        y = y_emb\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "\n",
    "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
    "        cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "            self.num_features\n",
    "        ) + off_diagonal(cov_y).pow_(2).sum().div(self.num_features)\n",
    "\n",
    "        loss = (\n",
    "            self.args.sim_coeff * repr_loss\n",
    "            + self.args.std_coeff * std_loss\n",
    "            + self.args.cov_coeff * cov_loss\n",
    "        )\n",
    "        if args.return_all_losses:\n",
    "            return loss, repr_loss, std_loss, cov_loss\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9a2d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.043091Z",
     "start_time": "2023-10-31T18:14:20.026611Z"
    }
   },
   "outputs": [],
   "source": [
    "def Projector(mlp, embedding):\n",
    "    mlp_spec = f\"{embedding}-{mlp}\"\n",
    "    layers = []\n",
    "    f = list(map(int, mlp_spec.split(\"-\")))\n",
    "    for i in range(len(f) - 2):\n",
    "        layers.append(nn.Linear(f[i], f[i + 1]))\n",
    "        layers.append(nn.BatchNorm1d(f[i + 1]))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(f[-2], f[-1], bias=False))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "def get_backbones(args):\n",
    "    x_backbone = Transformer(input_dim=args.x_inputs)\n",
    "    y_backbone = x_backbone if args.shared else copy.deepcopy(x_backbone)\n",
    "    return x_backbone, y_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14547f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.063754Z",
     "start_time": "2023-10-31T18:14:20.045646Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53a74e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.084149Z",
     "start_time": "2023-10-31T18:14:20.066170Z"
    }
   },
   "outputs": [],
   "source": [
    "args.mask = False\n",
    "args.cmask = True\n",
    "args.epoch = 10\n",
    "args.batch_size = 256\n",
    "args.label = \"notebook_test\"\n",
    "args.num_train_files = 1\n",
    "args.num_val_files = 1\n",
    "args.shared = False\n",
    "args.mlp = \"256-256-256\"\n",
    "args.transform_inputs = 32\n",
    "args.Do = 32\n",
    "args.hidden = 128\n",
    "args.sim_coeff = 25.0\n",
    "args.std_coeff = 25.0\n",
    "args.cov_coeff = 1.0\n",
    "args.return_embedding = False\n",
    "args.return_representation = False\n",
    "args.do_translation = True\n",
    "args.do_rotation = True\n",
    "args.do_cf = True\n",
    "args.do_ptd = True\n",
    "args.nconstit = 50\n",
    "args.ptst = 0.1\n",
    "args.ptcm = 0.1\n",
    "args.trsw = 1.0\n",
    "args.return_all_losses = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c88cb8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.179412Z",
     "start_time": "2023-10-31T18:14:20.086537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# define the global base device\n",
    "world_size = torch.cuda.device_count()\n",
    "if world_size:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    for i in range(world_size):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Device: CPU\")\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f1832",
   "metadata": {},
   "source": [
    "## Load a sample data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ffffc60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.894353Z",
     "start_time": "2023-10-31T18:14:20.182034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-31 18:14:20--  https://ssl-jet-v2.nrp-nautilus.io/toptagging/train/processed/3_features/data_0.pt\n",
      "Resolving ssl-jet-v2.nrp-nautilus.io (ssl-jet-v2.nrp-nautilus.io)... 198.32.43.49, 2605:d9c0:2:10::3:2\n",
      "Connecting to ssl-jet-v2.nrp-nautilus.io (ssl-jet-v2.nrp-nautilus.io)|198.32.43.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60001323 (57M) [application/vnd.snesdev-page-table]\n",
      "Saving to: ‘data_0.pt.3’\n",
      "\n",
      "data_0.pt.3         100%[===================>]  57.22M   171MB/s    in 0.3s    \n",
      "\n",
      "2023-10-31 18:14:20 (171 MB/s) - ‘data_0.pt.3’ saved [60001323/60001323]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://ssl-jet-v2.nrp-nautilus.io/toptagging/train/processed/3_features/data_0.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba07a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:20.940243Z",
     "start_time": "2023-10-31T18:14:20.900264Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = args.epoch\n",
    "batch_size = args.batch_size\n",
    "label = args.label\n",
    "\n",
    "# prepare data\n",
    "data = torch.load(\"data_0.pt\")\n",
    "data_train = data[:int(data.shape[0] * 0.8)]\n",
    "data_valid = data[int(data.shape[0] * 0.8):]\n",
    "\n",
    "n_train = len(data_train)\n",
    "n_val = len(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be9428d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:27.546564Z",
     "start_time": "2023-10-31T18:14:20.942494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VICReg(\n",
      "  (x_transform): Sequential(\n",
      "    (0): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (y_transform): Sequential(\n",
      "    (0): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (x_backbone): Transformer(\n",
      "    (embedding): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (head_layers): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (y_backbone): Transformer(\n",
      "    (embedding): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (head_layers): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (x_projector): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=False)\n",
      "  )\n",
      "  (y_projector): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=False)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 17.3429:  36%|█████████████████████████████████████████████▏                                                                                | 112/312 [00:05<00:09, 21.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch)\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 47\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     49\u001b[0m loss_train_batches\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.augmentation = augmentation\n",
    "\n",
    "args.x_inputs = 3\n",
    "args.y_inputs = 3\n",
    "\n",
    "args.x_backbone, args.y_backbone = get_backbones(args)\n",
    "model = VICReg(args).to(args.device)\n",
    "print(model)\n",
    "\n",
    "train_its = int(n_train / batch_size)\n",
    "val_its = int(n_val / batch_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_val_epochs = []  # loss recorded for each epoch\n",
    "repr_loss_val_epochs, std_loss_val_epochs, cov_loss_val_epochs = [], [], []\n",
    "# invariance, variance, covariance loss recorded for each epoch\n",
    "loss_val_batches = []  # loss recorded for each batch\n",
    "loss_train_epochs = []  # loss recorded for each epoch\n",
    "repr_loss_train_epochs, std_loss_train_epochs, cov_loss_train_epochs = [], [], []\n",
    "# invariance, variance, covariance loss recorded for each epoch\n",
    "loss_train_batches = []  # loss recorded for each batch\n",
    "l_val_best = 999999\n",
    "for m in range(n_epochs):\n",
    "    print(f\"Epoch {m}\\n\")\n",
    "    loss_train_epoch = []  # loss recorded for each batch in this epoch\n",
    "    repr_loss_train_epoch, std_loss_train_epoch, cov_loss_train_epoch = [], [], []\n",
    "    # invariance, variance, covariance loss recorded for each batch in this epoch\n",
    "    loss_val_epoch = []  # loss recorded for each batch in this epoch\n",
    "    repr_loss_val_epoch, std_loss_val_epoch, cov_loss_val_epoch = [], [], []\n",
    "    # invariance, variance, covariance loss recorded for each batch in this epoch\n",
    "\n",
    "    train_loader = DataLoader(data_train, batch_size)\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(train_loader, total=train_its)\n",
    "    for _, batch in enumerate(pbar):\n",
    "        batch = batch.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        if args.return_all_losses:\n",
    "            loss, repr_loss, std_loss, cov_loss = model.forward(batch)\n",
    "#             print(loss, repr_loss, std_loss, cov_loss)\n",
    "            repr_loss_train_epoch.append(repr_loss.detach().cpu().item())\n",
    "            std_loss_train_epoch.append(std_loss.detach().cpu().item())\n",
    "            cov_loss_train_epoch.append(cov_loss.detach().cpu().item())\n",
    "        else:\n",
    "            loss = model.forward(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.detach().cpu().item()\n",
    "        loss_train_batches.append(loss)\n",
    "        loss_train_epoch.append(loss)\n",
    "        pbar.set_description(f\"Training loss: {loss:.4f}\")\n",
    "    model.eval()\n",
    "    valid_loader = DataLoader(data_valid, batch_size)\n",
    "    pbar = tqdm.tqdm(valid_loader, total=val_its)\n",
    "    for _, batch in enumerate(pbar):\n",
    "        batch = batch.to(args.device)\n",
    "        if args.return_all_losses:\n",
    "            loss, repr_loss, std_loss, cov_loss = model.forward(batch)\n",
    "            repr_loss_val_epoch.append(repr_loss.detach().cpu().item())\n",
    "            std_loss_val_epoch.append(std_loss.detach().cpu().item())\n",
    "            cov_loss_val_epoch.append(cov_loss.detach().cpu().item())\n",
    "            loss = loss.detach().cpu().item()\n",
    "        else:\n",
    "            loss = model.forward(batch).cpu().item()\n",
    "        loss_val_batches.append(loss)\n",
    "        loss_val_epoch.append(loss)\n",
    "        pbar.set_description(f\"Validation loss: {loss:.4f}\")\n",
    "    l_val = np.mean(np.array(loss_val_epoch))\n",
    "    l_train = np.mean(np.array(loss_train_epoch))\n",
    "    loss_val_epochs.append(l_val)\n",
    "    loss_train_epochs.append(l_train)\n",
    "\n",
    "    if args.return_all_losses:\n",
    "        repr_l_val = np.mean(np.array(repr_loss_val_epoch))\n",
    "        repr_l_train = np.mean(np.array(repr_loss_train_epoch))\n",
    "        std_l_val = np.mean(np.array(std_loss_val_epoch))\n",
    "        std_l_train = np.mean(np.array(std_loss_train_epoch))\n",
    "        cov_l_val = np.mean(np.array(cov_loss_val_epoch))\n",
    "        cov_l_train = np.mean(np.array(cov_loss_train_epoch))\n",
    "\n",
    "        repr_loss_val_epochs.append(repr_l_val)\n",
    "        std_loss_val_epochs.append(std_l_val)\n",
    "        cov_loss_val_epochs.append(cov_l_val)\n",
    "\n",
    "        repr_loss_train_epochs.append(repr_l_train)\n",
    "        std_loss_train_epochs.append(std_l_train)\n",
    "        cov_loss_train_epochs.append(cov_l_train)\n",
    "    # save the model\n",
    "    if l_val < l_val_best:\n",
    "        print(\"New best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(model.state_dict(), f\"vicreg_{label}_best.pth\")\n",
    "    torch.save(model.state_dict(), f\"vicreg_{label}_last.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0604f17",
   "metadata": {},
   "source": [
    "## See the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3443891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:27.549037Z",
     "start_time": "2023-10-31T18:14:27.549020Z"
    }
   },
   "outputs": [],
   "source": [
    "fontsize = 10\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "ax[0, 0].plot(loss_train_epochs, 'r') #row=0, col=0\n",
    "ax[0, 0].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[0, 0].set_ylabel(\"Total loss\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "ax[1,0].plot(repr_loss_train_epochs, 'b') #row=1, col=0\n",
    "ax[1,0].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[1,0].set_ylabel(\"Invariance loss\", fontsize=fontsize)\n",
    "\n",
    "ax[0,1].plot(std_loss_train_epochs, 'g') #row=0, col=1\n",
    "ax[0,1].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[0,1].set_ylabel(\"Variance loss\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "ax[1,1].plot(cov_loss_train_epochs, 'y') #row=1, col=1\n",
    "ax[1,1].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[1,1].set_ylabel(\"Covariance loss\", fontsize=fontsize)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5) # adjust spacing between plots\n",
    "plt.figtext(0.5, 0.01, \"Different loss terms in training\", ha=\"center\", fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84f3ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:14:27.550665Z",
     "start_time": "2023-10-31T18:14:27.550649Z"
    }
   },
   "outputs": [],
   "source": [
    "fontsize = 10\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "ax[0, 0].plot(loss_val_epochs, 'r') #row=0, col=0\n",
    "ax[0, 0].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[0, 0].set_ylabel(\"Total loss\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "ax[1,0].plot(repr_loss_val_epochs, 'b') #row=1, col=0\n",
    "ax[1,0].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[1,0].set_ylabel(\"Invariance loss\", fontsize=fontsize)\n",
    "\n",
    "ax[0,1].plot(std_loss_val_epochs, 'g') #row=0, col=1\n",
    "ax[0,1].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[0,1].set_ylabel(\"Variance loss\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "ax[1,1].plot(cov_loss_val_epochs, 'y') #row=1, col=1\n",
    "ax[1,1].set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "ax[1,1].set_ylabel(\"Covariance loss\", fontsize=fontsize)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5) # adjust spacing between plots\n",
    "plt.figtext(0.5, 0.01, \"Different loss terms in validation\", ha=\"center\", fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20223ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
