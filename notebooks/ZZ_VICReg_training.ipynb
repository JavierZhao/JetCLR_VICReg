{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9a7fa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:50:11.191123Z",
     "start_time": "2023-08-10T21:50:00.260422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.10.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Building wheels for collected packages: torch_geometric\n",
      "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=fffb452be7a1221eda9109d82e4ba9885e2b67ba684edf0dfcfe60d8161caa21\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/aa/16/a8/fd7737d723cc1eb8df023c016c262ff4520091e1b022f8c164\n",
      "Successfully built torch_geometric\n",
      "Installing collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.3.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddc514b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T22:00:59.738697Z",
     "start_time": "2023-08-10T22:00:59.733862Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch import nn\n",
    "\n",
    "from src.models.transformer import Transformer\n",
    "from src.models.jet_augs import *\n",
    "from src.data.convert_data import convert_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051b659a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:51:51.183945Z",
     "start_time": "2023-08-10T21:51:51.180474Z"
    }
   },
   "outputs": [],
   "source": [
    "project_dir = \"/ssl-jet-vol-v2/JetCLR_VICReg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19547c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T17:31:39.703669Z",
     "start_time": "2023-07-27T17:31:39.688519Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a771da83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:51:53.739822Z",
     "start_time": "2023-08-10T21:51:53.735146Z"
    }
   },
   "outputs": [],
   "source": [
    "def contains_nan(tensor):\n",
    "    has_nan = torch.isnan(tensor)\n",
    "    return torch.any(has_nan).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f81d6ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:52:10.665411Z",
     "start_time": "2023-08-10T21:52:10.653184Z"
    }
   },
   "outputs": [],
   "source": [
    "class VICReg(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.num_features = int(\n",
    "            args.mlp.split(\"-\")[-1]\n",
    "        )  # size of the last layer of the MLP projector\n",
    "        self.x_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.x_inputs),\n",
    "            nn.Linear(args.x_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.y_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.y_inputs),\n",
    "            nn.Linear(args.y_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.augmentation = args.augmentation\n",
    "        self.x_backbone = args.x_backbone\n",
    "        self.y_backbone = args.y_backbone\n",
    "        self.N_x = self.x_backbone.input_dim\n",
    "        self.N_y = self.y_backbone.input_dim\n",
    "        self.embedding = args.Do\n",
    "        self.return_embedding = args.return_embedding\n",
    "        self.return_representation = args.return_representation\n",
    "        self.x_projector = Projector(args.mlp, self.embedding)\n",
    "        self.y_projector = (\n",
    "            self.x_projector if args.shared else copy.deepcopy(self.x_projector)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x -> x_aug -> (x_xform) -> x_rep -> x_emb\n",
    "        y -> y_aug -> (y_xform) -> y_rep -> y_emb\n",
    "        _aug: augmented\n",
    "        _xform: transformed by linear layer (skipped because it destroys the zero padding)\n",
    "        _rep: backbone representation\n",
    "        _emb: projected embedding\n",
    "        \"\"\"\n",
    "        x_aug, y_aug = self.augmentation(\n",
    "            self.args, x, self.args.device\n",
    "        )  # [batch_size, n_constit, 3]\n",
    "#         print(f\"x_aug contains nan: {contains_nan(x_aug)}\")\n",
    "#         print(f\"y_aug contains nan: {contains_nan(y_aug)}\")\n",
    "\n",
    "        # x_xform = self.x_transform.to(torch.double)(\n",
    "        #     x_aug.x.double()\n",
    "        # )  # [batch_size, n_constit, transform_inputs]?\n",
    "        # y_xform = self.y_transform.to(torch.double)(\n",
    "        #     y_aug.x.double()\n",
    "        # )  # [batch_size, n_constit, transform_inputs]?\n",
    "\n",
    "        x_rep = self.x_backbone(\n",
    "            x_aug, use_mask=self.args.mask, use_continuous_mask=self.args.cmask\n",
    "        )  # [batch_size, output_dim]\n",
    "        y_rep = self.y_backbone(\n",
    "            y_aug, use_mask=self.args.mask, use_continuous_mask=self.args.cmask\n",
    "        )  # [batch_size, output_dim]\n",
    "#         print(f\"x_rep contains nan: {contains_nan(x_rep)}\")\n",
    "#         print(f\"y_rep contains nan: {contains_nan(y_rep)}\")\n",
    "        if self.return_representation:\n",
    "            return x_rep, y_rep\n",
    "\n",
    "        x_emb = self.x_projector(x_rep)  # [batch_size, embedding_size]\n",
    "        y_emb = self.y_projector(y_rep)  # [batch_size, embedding_size]\n",
    "#         print(f\"x_emb contains nan: {contains_nan(x_emb)}\")\n",
    "#         print(f\"y_emb contains nan: {contains_nan(y_emb)}\")\n",
    "        if self.return_embedding:\n",
    "            return x_emb, y_emb\n",
    "        x = x_emb\n",
    "        y = y_emb\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "\n",
    "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
    "        cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "            self.num_features\n",
    "        ) + off_diagonal(cov_y).pow_(2).sum().div(self.num_features)\n",
    "\n",
    "        loss = (\n",
    "            self.args.sim_coeff * repr_loss\n",
    "            + self.args.std_coeff * std_loss\n",
    "            + self.args.cov_coeff * cov_loss\n",
    "        )\n",
    "        if args.return_all_losses:\n",
    "            return loss, repr_loss, std_loss, cov_loss\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ea8478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:56:56.529169Z",
     "start_time": "2023-08-10T21:56:56.513388Z"
    }
   },
   "outputs": [],
   "source": [
    "def Projector(mlp, embedding):\n",
    "    mlp_spec = f\"{embedding}-{mlp}\"\n",
    "    layers = []\n",
    "    f = list(map(int, mlp_spec.split(\"-\")))\n",
    "    for i in range(len(f) - 2):\n",
    "        layers.append(nn.Linear(f[i], f[i + 1]))\n",
    "        layers.append(nn.BatchNorm1d(f[i + 1]))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(f[-2], f[-1], bias=False))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "def get_backbones(args):\n",
    "    x_backbone = Transformer(input_dim=args.x_inputs)\n",
    "    y_backbone = x_backbone if args.shared else copy.deepcopy(x_backbone)\n",
    "    return x_backbone, y_backbone\n",
    "\n",
    "\n",
    "def augmentation(args, x, device):\n",
    "    \"\"\"\n",
    "    Applies all the augmentations specified in the args\n",
    "    \"\"\"\n",
    "    # cropping to 50 particles is already done in data preprocessing\n",
    "    # crop all jets to a fixed number of constituents (default=50)\n",
    "#     x = crop_jets(x, args.nconstit)\n",
    "    x = rotate_jets(x, device)\n",
    "    y = x.clone()\n",
    "    if args.do_rotation:\n",
    "        y = rotate_jets(y, device)\n",
    "    if args.do_cf:\n",
    "        y = collinear_fill_jets(np.array(y.cpu()), device)\n",
    "        y = collinear_fill_jets(np.array(y.cpu()), device)\n",
    "    if args.do_ptd:\n",
    "        y = distort_jets(y, device, strength=args.ptst, pT_clip_min=args.ptcm)\n",
    "    if args.do_translation:\n",
    "        y = translate_jets(y, device, width=args.trsw)\n",
    "        x = translate_jets(x, device, width=args.trsw)\n",
    "    x = rescale_pts(x)  # [batch_size, 3, n_constit]\n",
    "    y = rescale_pts(y)  # [batch_size, 3, n_constit]\n",
    "    x = x.transpose(1, 2)  # [batch_size, 3, n_constit] -> [batch_size, n_constit, 3]\n",
    "    y = y.transpose(1, 2)  # [batch_size, 3, n_constit] -> [batch_size, n_constit, 3]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# load the datafiles\n",
    "def load_data(dataset_path, flag, n_files=-1):\n",
    "    data_files = glob.glob(f\"{dataset_path}/{flag}/processed/3_features/*\")\n",
    "\n",
    "    data = []\n",
    "    for i, file in enumerate(data_files):\n",
    "        data += torch.load(f\"{dataset_path}/{flag}/processed/3_features/data_{i}.pt\")\n",
    "        print(f\"--- loaded file {i} from `{flag}` directory\")\n",
    "        if n_files != -1 and i == n_files - 1:\n",
    "            break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e26e304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:52:24.232863Z",
     "start_time": "2023-08-10T21:52:24.227222Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11233735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T22:06:54.533072Z",
     "start_time": "2023-08-10T22:06:54.524052Z"
    }
   },
   "outputs": [],
   "source": [
    "args.mask = False\n",
    "args.cmask = True\n",
    "args.epoch = 10\n",
    "args.batch_size = 256\n",
    "args.outdir = f\"{project_dir}/models/\"\n",
    "args.label = \"notebook_test\"\n",
    "args.dataset_path = \"/ssl-jet-vol-v2/toptagging\"\n",
    "args.num_train_files = 1\n",
    "args.num_val_files = 1\n",
    "args.shared = False\n",
    "args.mlp = \"256-256-256\"\n",
    "args.transform_inputs = 32\n",
    "args.Do = 1000\n",
    "args.hidden = 128\n",
    "args.sim_coeff = 25.0\n",
    "args.std_coeff = 25.0\n",
    "args.cov_coeff = 1.0\n",
    "args.return_embedding = False\n",
    "args.return_representation = False\n",
    "args.do_translation = True\n",
    "args.do_rotation = True\n",
    "args.do_cf = True\n",
    "args.do_ptd = True\n",
    "args.nconstit = 50\n",
    "args.ptst = 0.1\n",
    "args.ptcm = 0.1\n",
    "args.trsw = 1.0\n",
    "args.return_all_losses = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2cf6f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:52:46.738045Z",
     "start_time": "2023-08-10T21:52:46.729034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA A100 80GB PCIe MIG 1g.10gb\n"
     ]
    }
   ],
   "source": [
    "# define the global base device\n",
    "world_size = torch.cuda.device_count()\n",
    "if world_size:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    for i in range(world_size):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Device: CPU\")\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a01d16c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:57:05.387795Z",
     "start_time": "2023-08-10T21:57:01.563732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- loaded file 0 from `train` directory\n",
      "--- loaded file 0 from `val` directory\n"
     ]
    }
   ],
   "source": [
    "n_epochs = args.epoch\n",
    "batch_size = args.batch_size\n",
    "outdir = args.outdir\n",
    "label = args.label\n",
    "\n",
    "\n",
    "model_loc = f\"{outdir}/trained_models/\"\n",
    "model_perf_loc = f\"{outdir}/model_performances/{label}\"\n",
    "model_dict_loc = f\"{outdir}/model_dicts/\"\n",
    "os.system(\n",
    "    f\"mkdir -p {model_loc} {model_perf_loc} {model_dict_loc}\"\n",
    ")  # -p: create parent dirs if needed, exist_ok\n",
    "\n",
    "# prepare data\n",
    "data_train = load_data(args.dataset_path, \"train\", n_files=args.num_train_files)\n",
    "data_valid = load_data(args.dataset_path, \"val\", n_files=args.num_val_files)\n",
    "\n",
    "n_train = len(data_train)\n",
    "n_val = len(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a29a9c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:59:55.119931Z",
     "start_time": "2023-08-10T21:59:55.115192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0209d23d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:59:21.730165Z",
     "start_time": "2023-08-10T21:59:21.674769Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file = torch.load(f\"{args.dataset_path}/train/processed/3_features/data_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b687e45d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T21:59:29.333190Z",
     "start_time": "2023-08-10T21:59:29.328623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100001, 3, 50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13172e05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T22:01:21.369890Z",
     "start_time": "2023-08-10T22:01:21.363628Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ccae934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T22:01:48.888536Z",
     "start_time": "2023-08-10T22:01:48.883463Z"
    }
   },
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cffde0e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T22:02:00.462791Z",
     "start_time": "2023-08-10T22:02:00.458396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 50])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69e95901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T22:07:22.128860Z",
     "start_time": "2023-08-10T22:07:05.252444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.6040:   0%|▏                                                                         | 1/390 [00:02<13:15,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 22.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.8887:   1%|▍                                                                         | 2/390 [00:04<13:02,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 22.8887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.6074:   1%|▌                                                                         | 3/390 [00:06<12:57,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 22.6074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.2898:   1%|▊                                                                         | 4/390 [00:08<12:47,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 22.2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.1764:   1%|▉                                                                         | 5/390 [00:09<12:47,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 22.1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.0609:   2%|█▏                                                                        | 6/390 [00:12<13:17,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 22.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.0753:   2%|█▎                                                                        | 7/390 [00:14<13:06,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 22.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss: 22.0753:   2%|█▎                                                                        | 7/390 [00:15<14:04,  2.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch)\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 47\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     49\u001b[0m loss_train_batches\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:262\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    259\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.augmentation = augmentation\n",
    "\n",
    "args.x_inputs = 3\n",
    "args.y_inputs = 3\n",
    "\n",
    "args.x_backbone, args.y_backbone = get_backbones(args)\n",
    "model = VICReg(args).to(args.device)\n",
    "\n",
    "train_its = int(n_train / batch_size)\n",
    "val_its = int(n_val / batch_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_val_epochs = []  # loss recorded for each epoch\n",
    "repr_loss_val_epochs, std_loss_val_epochs, cov_loss_val_epochs = [], [], []\n",
    "# invariance, variance, covariance loss recorded for each epoch\n",
    "loss_val_batches = []  # loss recorded for each batch\n",
    "loss_train_epochs = []  # loss recorded for each epoch\n",
    "repr_loss_train_epochs, std_loss_train_epochs, cov_loss_train_epochs = [], [], []\n",
    "# invariance, variance, covariance loss recorded for each epoch\n",
    "loss_train_batches = []  # loss recorded for each batch\n",
    "l_val_best = 999999\n",
    "for m in range(n_epochs):\n",
    "    print(f\"Epoch {m}\\n\")\n",
    "    loss_train_epoch = []  # loss recorded for each batch in this epoch\n",
    "    repr_loss_train_epoch, std_loss_train_epoch, cov_loss_train_epoch = [], [], []\n",
    "    # invariance, variance, covariance loss recorded for each batch in this epoch\n",
    "    loss_val_epoch = []  # loss recorded for each batch in this epoch\n",
    "    repr_loss_val_epoch, std_loss_val_epoch, cov_loss_val_epoch = [], [], []\n",
    "    # invariance, variance, covariance loss recorded for each batch in this epoch\n",
    "\n",
    "    train_loader = DataLoader(data_train, batch_size)\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(train_loader, total=train_its)\n",
    "#     for _, batch in tqdm.tqdm(enumerate(train_loader)):\n",
    "    for _, batch in enumerate(pbar):\n",
    "        batch = batch.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        if args.return_all_losses:\n",
    "            loss, repr_loss, std_loss, cov_loss = model.forward(batch)\n",
    "#             print(loss, repr_loss, std_loss, cov_loss)\n",
    "            repr_loss_train_epoch.append(repr_loss.detach().cpu().item())\n",
    "            std_loss_train_epoch.append(std_loss.detach().cpu().item())\n",
    "            cov_loss_train_epoch.append(cov_loss.detach().cpu().item())\n",
    "        else:\n",
    "            loss = model.forward(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.detach().cpu().item()\n",
    "        loss_train_batches.append(loss)\n",
    "        loss_train_epoch.append(loss)\n",
    "        pbar.set_description(f\"Training loss: {loss:.4f}\")\n",
    "        print(f\"Training loss: {loss:.4f}\")\n",
    "    model.eval()\n",
    "    valid_loader = DataLoader(data_valid, batch_size)\n",
    "    pbar = tqdm.tqdm(valid_loader, total=val_its)\n",
    "#     for _, batch in tqdm.tqdm(enumerate(valid_loader)):\n",
    "    for _, batch in enumerate(pbar):\n",
    "        batch = batch.to(args.device)\n",
    "        batch = convert_x(batch, args.device)  # [batch_size, 3, n_constit]\n",
    "        if args.return_all_losses:\n",
    "            loss, repr_loss, std_loss, cov_loss = model.forward(batch)\n",
    "            repr_loss_val_epoch.append(repr_loss.detach().cpu().item())\n",
    "            std_loss_val_epoch.append(std_loss.detach().cpu().item())\n",
    "            cov_loss_val_epoch.append(cov_loss.detach().cpu().item())\n",
    "            loss = loss.detach().cpu().item()\n",
    "        else:\n",
    "            loss = model.forward(batch).cpu().item()\n",
    "        loss_val_batches.append(loss)\n",
    "        loss_val_epoch.append(loss)\n",
    "        pbar.set_description(f\"Validation loss: {loss:.4f}\")\n",
    "        print(f\"Validation loss: {loss:.4f}\")\n",
    "    l_val = np.mean(np.array(loss_val_epoch))\n",
    "    l_train = np.mean(np.array(loss_train_epoch))\n",
    "    loss_val_epochs.append(l_val)\n",
    "    loss_train_epochs.append(l_train)\n",
    "\n",
    "    if args.return_all_losses:\n",
    "        repr_l_val = np.mean(np.array(repr_loss_val_epoch))\n",
    "        repr_l_train = np.mean(np.array(repr_loss_train_epoch))\n",
    "        std_l_val = np.mean(np.array(std_loss_val_epoch))\n",
    "        std_l_train = np.mean(np.array(std_loss_train_epoch))\n",
    "        cov_l_val = np.mean(np.array(cov_loss_val_epoch))\n",
    "        cov_l_train = np.mean(np.array(cov_loss_train_epoch))\n",
    "\n",
    "        repr_loss_val_epochs.append(repr_l_val)\n",
    "        std_loss_val_epochs.append(std_l_val)\n",
    "        cov_loss_val_epochs.append(cov_l_val)\n",
    "\n",
    "        repr_loss_train_epochs.append(repr_l_train)\n",
    "        std_loss_train_epochs.append(std_l_train)\n",
    "        cov_loss_train_epochs.append(cov_l_train)\n",
    "    # save the model\n",
    "    if l_val < l_val_best:\n",
    "        print(\"New best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(model.state_dict(), f\"{model_loc}/vicreg_{label}_best.pth\")\n",
    "    torch.save(model.state_dict(), f\"{model_loc}/vicreg_{label}_last.pth\")\n",
    "# After training\n",
    "\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_train_epochs.npy\",\n",
    "    np.array(loss_train_epochs),\n",
    ")\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_train_batches.npy\",\n",
    "    np.array(loss_train_batches),\n",
    ")\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_val_epochs.npy\",\n",
    "    np.array(loss_val_epochs),\n",
    ")\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_val_batches.npy\",\n",
    "    np.array(loss_val_batches),\n",
    ")\n",
    "if args.return_all_losses:\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_repr_loss_train_epochs.npy\",\n",
    "        np.array(repr_loss_train_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_std_loss_train_epochs.npy\",\n",
    "        np.array(std_loss_train_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_cov_loss_train_epochs.npy\",\n",
    "        np.array(cov_loss_train_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_repr_loss_val_epochs.npy\",\n",
    "        np.array(repr_loss_val_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_std_loss_val_epochs.npy\",\n",
    "        np.array(std_loss_val_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_cov_loss_val_epochs.npy\",\n",
    "        np.array(cov_loss_val_epochs),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bab4c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T20:28:26.671719Z",
     "start_time": "2023-08-04T20:28:26.671710Z"
    }
   },
   "source": [
    "## Inspect augmented jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fde6fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:18:42.138229Z",
     "start_time": "2023-08-04T21:18:42.138219Z"
    }
   },
   "outputs": [],
   "source": [
    "for _, batch in tqdm.tqdm(enumerate(train_loader)):\n",
    "    batch = batch.to(args.device)\n",
    "    batch = convert_x(batch, args.device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfc927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:18:42.139574Z",
     "start_time": "2023-08-04T21:18:42.139565Z"
    }
   },
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf621f99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:18:42.140473Z",
     "start_time": "2023-08-04T21:18:42.140465Z"
    }
   },
   "outputs": [],
   "source": [
    "x_aug, y_aug = augmentation(args, batch, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff31ddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:18:42.141517Z",
     "start_time": "2023-08-04T21:18:42.141508Z"
    }
   },
   "outputs": [],
   "source": [
    "x_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feeab22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:18:42.142321Z",
     "start_time": "2023-08-04T21:18:42.142313Z"
    }
   },
   "outputs": [],
   "source": [
    "has_nan = torch.isnan(y_aug)\n",
    "contains_nan = torch.any(has_nan)\n",
    "print(contains_nan.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f2e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:18:42.143378Z",
     "start_time": "2023-08-04T21:18:42.143368Z"
    }
   },
   "outputs": [],
   "source": [
    "contains_nan(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b7098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:18:42.144431Z",
     "start_time": "2023-08-04T21:18:42.144421Z"
    }
   },
   "outputs": [],
   "source": [
    "model.forward(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
