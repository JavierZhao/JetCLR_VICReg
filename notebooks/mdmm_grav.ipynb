{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mdmm` tutorial\n",
    "The purpose of this notebook is to guide you to train a model with a loss function of several parameters in a proper mathematical way. The idea comes from the paper [Constrained Differential Optimization](https://papers.nips.cc/paper/1987/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf), and the implementation from the [mdmm package Github](https://github.com/crowsonkb/mdmm)\n",
    "\n",
    "The guide is best illustrated through the **[VICReg](https://arxiv.org/abs/2105.04906)** example where your input is split into two views and you are asked to minimize three loss terms: `variance`, `invariance` and `covariance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:30:25.498537Z",
     "start_time": "2023-07-21T17:30:20.593468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mdmm\n",
      "  Downloading mdmm-0.1.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from mdmm) (1.13.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->mdmm) (4.5.0)\n",
      "Installing collected packages: mdmm\n",
      "Successfully installed mdmm-0.1.3\n"
     ]
    }
   ],
   "source": [
    "! pip install mdmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:30:34.557292Z",
     "start_time": "2023-07-21T17:30:26.516905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import mdmm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.conv import GravNetConv\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# define the global base device\n",
    "if torch.cuda.device_count():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Will use {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Will use cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a processed `.pt` clic file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:10.569421Z",
     "start_time": "2023-07-21T17:30:40.583902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clic events 100001\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"/../ssl-jet-vol-v2/toptagging/train/processed/data_0.pt\")\n",
    "print(f\"num of clic events {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:55.967066Z",
     "start_time": "2023-07-21T17:31:55.896955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single event: \n",
      " DataBatch(x=[2342, 7], y=[50], batch=[2342], ptr=[51])\n"
     ]
    }
   ],
   "source": [
    "# build a data loader\n",
    "batch_size = 50\n",
    "\n",
    "loader = DataLoader(data, batch_size, shuffle=True)\n",
    "for batch in loader:\n",
    "    print(f\"A single event: \\n {batch}\")\n",
    "    break\n",
    "\n",
    "input_dim = batch.x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.020921Z",
     "start_time": "2023-07-21T17:31:55.969842Z"
    }
   },
   "outputs": [],
   "source": [
    "def event_augmentation(batch):\n",
    "    \"\"\"\n",
    "    Takes events of the form Batch() and splits them into two Batch() objects representing the two views.\n",
    "\n",
    "    In this example, the first view is tracks and the second view is clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    is_track = batch.x[:, 0] == 1\n",
    "    is_cluster = batch.x[:, 0] == 2\n",
    "\n",
    "    view1 = Batch(\n",
    "        x=batch.x[is_track],\n",
    "        ygen=batch.ygen[is_track],\n",
    "        ygen_id=batch.ygen_id[is_track],\n",
    "        ycand=batch.ycand[is_track],\n",
    "        ycand_id=batch.ycand_id[is_track],\n",
    "        batch=batch.batch[is_track],\n",
    "    )\n",
    "    view2 = Batch(\n",
    "        x=batch.x[is_cluster],\n",
    "        ygen=batch.ygen[is_cluster],\n",
    "        ygen_id=batch.ygen_id[is_cluster],\n",
    "        ycand=batch.ycand[is_cluster],\n",
    "        ycand_id=batch.ycand_id[is_cluster],\n",
    "        batch=batch.batch[is_cluster],\n",
    "    )\n",
    "\n",
    "    return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.778632Z",
     "start_time": "2023-07-21T17:31:56.023536Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'ygen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/data/storage.py:79\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/data/storage.py:104\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ygen'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m view1, view2 \u001b[38;5;241m=\u001b[39m \u001b[43mevent_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mview1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mview2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mview2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mevent_augmentation\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m is_track \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mx[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m is_cluster \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mx[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     11\u001b[0m view1 \u001b[38;5;241m=\u001b[39m Batch(\n\u001b[1;32m     12\u001b[0m     x\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mx[is_track],\n\u001b[0;32m---> 13\u001b[0m     ygen\u001b[38;5;241m=\u001b[39m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mygen\u001b[49m[is_track],\n\u001b[1;32m     14\u001b[0m     ygen_id\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mygen_id[is_track],\n\u001b[1;32m     15\u001b[0m     ycand\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mycand[is_track],\n\u001b[1;32m     16\u001b[0m     ycand_id\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mycand_id[is_track],\n\u001b[1;32m     17\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch[is_track],\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m view2 \u001b[38;5;241m=\u001b[39m Batch(\n\u001b[1;32m     20\u001b[0m     x\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mx[is_cluster],\n\u001b[1;32m     21\u001b[0m     ygen\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mygen[is_cluster],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch[is_cluster],\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m view1, view2\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/data/data.py:441\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/data/storage.py:81\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'ygen'"
     ]
    }
   ],
   "source": [
    "view1, view2 = event_augmentation(batch)\n",
    "print(f\"view1: {view1}\")\n",
    "print(f\"view2: {view2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the VICReg model (GravNet-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.782266Z",
     "start_time": "2023-07-21T17:31:56.782242Z"
    }
   },
   "outputs": [],
   "source": [
    "class VICReg(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VICReg, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.augmentation = event_augmentation\n",
    "\n",
    "    def forward(self, event):\n",
    "        # seperate the two views\n",
    "        view1, view2 = self.augmentation(event)\n",
    "\n",
    "        # encode to retrieve the representations\n",
    "        view1_representations, view2_representations = self.encoder(view1, view2)\n",
    "\n",
    "        # simple MLP decoder\n",
    "        view1_embeddings = self.decoder(view1_representations)\n",
    "        view2_embeddings = self.decoder(view2_representations)\n",
    "\n",
    "        # global pooling to be able to compute a loss between views of different dimensionalities\n",
    "        view1_embeddings = global_mean_pool(view1_embeddings, view1.batch)\n",
    "        view2_embeddings = global_mean_pool(view2_embeddings, view2.batch)\n",
    "\n",
    "        return view1_embeddings, view2_embeddings\n",
    "\n",
    "\n",
    "class ENCODER(nn.Module):\n",
    "    \"\"\"The Encoder part of VICReg which attempts to learn useful latent representations of the two views.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        width=126,\n",
    "        embedding_dim=34,\n",
    "        num_convs=2,\n",
    "    ):\n",
    "        super(ENCODER, self).__init__()\n",
    "\n",
    "        self.act = nn.ELU\n",
    "\n",
    "        # 1. different MLP for each view\n",
    "        self.nn1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, embedding_dim),\n",
    "        )\n",
    "        self.nn2 = nn.Sequential(\n",
    "            nn.Linear(17, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, embedding_dim),\n",
    "        )\n",
    "\n",
    "        # 2. same GNN for each view\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_convs):\n",
    "            self.convs.append(\n",
    "                GravNetConv(\n",
    "                    embedding_dim,\n",
    "                    embedding_dim,\n",
    "                    space_dimensions=4,\n",
    "                    propagate_dimensions=22,\n",
    "                    k=8,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, view1, view2):\n",
    "        view1_representations = self.nn1(view1.x.float())\n",
    "        view2_representations = self.nn2(view2.x.float())\n",
    "\n",
    "        # perform a series of graph convolutions\n",
    "        for num, conv in enumerate(self.convs):\n",
    "            view1_representations = conv(view1_representations, view1.batch)\n",
    "            view2_representations = conv(view2_representations, view2.batch)\n",
    "\n",
    "        return view1_representations, view2_representations\n",
    "\n",
    "\n",
    "class DECODER(nn.Module):\n",
    "    \"\"\"The Decoder part of VICReg which attempts to expand the learned latent representations\n",
    "    of the two views into a space where a loss can be computed.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=34,\n",
    "        width=126,\n",
    "        output_dim=200,\n",
    "    ):\n",
    "        super(DECODER, self).__init__()\n",
    "\n",
    "        self.act = nn.ELU\n",
    "\n",
    "        # DECODER\n",
    "        self.expander = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.expander(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.784557Z",
     "start_time": "2023-07-21T17:31:56.784533Z"
    }
   },
   "outputs": [],
   "source": [
    "vicreg_encoder = ENCODER(input_dim, embedding_dim=34)\n",
    "vicreg_decoder = DECODER(embedding_dim=34, output_dim=200)\n",
    "\n",
    "vicreg = VICReg(vicreg_encoder, vicreg_decoder)\n",
    "vicreg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the loss terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.786401Z",
     "start_time": "2023-07-21T17:31:56.786377Z"
    }
   },
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    \"\"\"Copied from VICReg paper github https://github.com/facebookresearch/vicreg/\"\"\"\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "class CovLoss(nn.Module):\n",
    "    def forward(self, view1, view2):\n",
    "        N = view1.size(0)  # batch size\n",
    "        D = view1.size(1)  # dim of representations\n",
    "\n",
    "        view1 = view1 - view1.mean(dim=0)\n",
    "        view2 = view2 - view2.mean(dim=0)\n",
    "\n",
    "        cov_view1 = (view1.T @ view1) / (N - 1)\n",
    "        cov_view2 = (view2.T @ view2) / (N - 1)\n",
    "\n",
    "        loss = off_diagonal(cov_view1).pow_(2).sum().div(D) + off_diagonal(cov_view2).pow_(2).sum().div(D)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class VarLoss(nn.Module):\n",
    "    def forward(self, view1, view2):\n",
    "        view1 = view1 - view1.mean(dim=0)\n",
    "        view2 = view2 - view2.mean(dim=0)\n",
    "\n",
    "        # variance loss\n",
    "        std_view1 = torch.sqrt(view1.var(dim=0) + 1e-10)\n",
    "        std_view2 = torch.sqrt(view2.var(dim=0) + 1e-10)\n",
    "\n",
    "        loss = torch.mean(F.relu(1 - std_view1)) / 2 + torch.mean(F.relu(1 - std_view2)) / 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.788378Z",
     "start_time": "2023-07-21T17:31:56.788354Z"
    }
   },
   "outputs": [],
   "source": [
    "crit_invar = nn.MSELoss()\n",
    "crit_var = VarLoss()\n",
    "crit_cov = CovLoss()\n",
    "\n",
    "max_var = 1e-5 * batch_size\n",
    "max_cov = 50 * batch_size\n",
    "\n",
    "constraints = []\n",
    "constraints.append(mdmm.MaxConstraint(lambda: crit_var(view1_embeddings, view2_embeddings), max_var))\n",
    "constraints.append(mdmm.MaxConstraint(lambda: crit_cov(view1_embeddings, view2_embeddings), max_cov, scale=1e4))\n",
    "\n",
    "mdmm_module = mdmm.MDMM(constraints)\n",
    "optimizer = mdmm_module.make_optimizer(vicreg.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.790265Z",
     "start_time": "2023-07-21T17:31:56.790241Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(loader):\n",
    "#     # run VICReg forward pass to get the embeddings\n",
    "#     view1_embeddings, view2_embeddings = vicreg(batch.to(device))\n",
    "\n",
    "#     # compute the invariance loss which is contrained by the other loss terms\n",
    "#     loss = batch_size * crit_invar(view1_embeddings, view2_embeddings)\n",
    "#     mdmm_return = mdmm_module(loss)\n",
    "\n",
    "#     # backprop\n",
    "#     for param in vicreg.parameters():\n",
    "#         param.grad = None\n",
    "#     mdmm_return.value.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "#     print(f\"constrained invariance loss: {loss.detach():.2f}\")\n",
    "\n",
    "#     if i == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-21T17:31:56.792412Z",
     "start_time": "2023-07-21T17:31:56.792388Z"
    }
   },
   "outputs": [],
   "source": [
    "losses_inv, losses_var, losses_cov, losses_reg = [], [], [], []\n",
    "print(f\"max var: {max_var}\")\n",
    "print(f\"max cov: {max_cov}\")\n",
    "for i, batch in enumerate(loader):\n",
    "    print(f\"batch {i}\")\n",
    "    # run VICReg forward pass to get the embeddings\n",
    "    view1_embeddings, view2_embeddings = vicreg(batch.to(device))\n",
    "#     print(f\"view1_embeddings size: {view1_embeddings.size()}\")\n",
    "\n",
    "    # compute the invariance loss which is contrained by the other loss terms\n",
    "    loss = batch_size * crit_invar(view1_embeddings, view2_embeddings)\n",
    "    print(f\"invariance loss: {loss:.2f}\")\n",
    "    losses_inv.append(loss.detach().item())\n",
    "    loss_var = batch_size * crit_var(view1_embeddings, view2_embeddings)\n",
    "    print(f\"variance loss: {loss_var:.2f}\")\n",
    "    losses_var.append(loss_var.detach().item())\n",
    "    loss_cov = batch_size * crit_cov(view1_embeddings, view2_embeddings)\n",
    "    print(f\"covariance loss: {loss_cov:.2f}\")\n",
    "    losses_cov.append(loss_cov.detach().item())\n",
    "\n",
    "\n",
    "    mdmm_return = mdmm_module(loss.to(device))\n",
    "\n",
    "    # backprop\n",
    "    for param in vicreg.parameters():\n",
    "        param.grad = None\n",
    "    mdmm_return.value.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    print(f\"constrained invariance loss: {loss.detach():.2f}\")\n",
    "    losses_reg.append(loss.detach().item())\n",
    "\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(\"-----------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd710be4164d8116e60481776a482d6ed163c0c31d42101b2cd55e4bfc6d2c5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
