{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mdmm` tutorial\n",
    "The purpose of this notebook is to guide you to train a model with a loss function of several parameters in a proper mathematical way. The idea comes from the paper [Constrained Differential Optimization](https://papers.nips.cc/paper/1987/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf), and the implementation from the [mdmm package Github](https://github.com/crowsonkb/mdmm)\n",
    "\n",
    "The guide is best illustrated through the **[VICReg](https://arxiv.org/abs/2105.04906)** example where your input is split into two views and you are asked to minimize three loss terms: `variance`, `invariance` and `covariance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:48:20.638413Z",
     "start_time": "2023-07-19T22:48:16.898736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mdmm in /opt/conda/lib/python3.10/site-packages (0.1.3)\r\n",
      "Requirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from mdmm) (1.13.1)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->mdmm) (4.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install mdmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:48:23.073840Z",
     "start_time": "2023-07-19T22:48:20.643177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import mdmm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, GATConv\n",
    "from torch_geometric.nn.conv import GravNetConv\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# define the global base device\n",
    "if torch.cuda.device_count():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Will use {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Will use cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:48:23.081549Z",
     "start_time": "2023-07-19T22:48:23.077878Z"
    }
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a processed `.pt` clic file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.293984Z",
     "start_time": "2023-07-19T22:48:23.084651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clic events 100001\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"/../ssl-jet-vol-v2/toptagging/train/processed/data_0.pt\")\n",
    "print(f\"num of clic events {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.358085Z",
     "start_time": "2023-07-19T22:49:04.297949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single event: \n",
      " DataBatch(x=[6210, 7], y=[128], batch=[6210], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "# build a data loader\n",
    "batch_size = 128\n",
    "\n",
    "loader = DataLoader(data, batch_size, shuffle=True)\n",
    "for batch in loader:\n",
    "    print(f\"A single event: \\n {batch}\")\n",
    "    break\n",
    "\n",
    "input_dim = batch.x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.378094Z",
     "start_time": "2023-07-19T22:49:04.361422Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate_jets(batch, width=1.0, device='cpu'):\n",
    "    width = 1.0\n",
    "    bb = batch.clone()\n",
    "    bb = bb.to(\"cpu\")\n",
    "    X = bb.x.numpy()\n",
    "    ptp_eta = np.ptp(X[:,0], axis=-1, keepdims=True)\n",
    "    ptp_phi = np.ptp(X[:,1], axis=-1, keepdims=True)\n",
    "    low_eta  = -width*ptp_eta\n",
    "#     print(f\"low eta: {low_eta}\")\n",
    "    high_eta = +width*ptp_eta\n",
    "#     print(f\"high eta: {high_eta}\")\n",
    "    low_phi = np.maximum(-width*ptp_phi, -np.pi-np.min(X[:,1]).reshape(ptp_phi.shape))\n",
    "#     print(f\"low phi: {low_phi}\")\n",
    "    high_phi = np.minimum(+width*ptp_phi, +np.pi-np.max(X[:,1]).reshape(ptp_phi.shape))\n",
    "#     print(f\"high phi: {high_phi}\")\n",
    "    shift_eta_batch = np.random.uniform(low=low_eta, high=high_eta, size=(bb.y.shape[0], 1))  \n",
    "    shift_phi_batch = np.random.uniform(low=low_phi, high=high_phi, size=(bb.y.shape[0], 1))  \n",
    "    \n",
    "    # To make sure that the components of each jet get shifted by the same amount\n",
    "    for i in range(len(bb)):\n",
    "        X_jet = bb[i].x.numpy()\n",
    "        shift_eta_jet = np.ones((X_jet.shape[0], 1)) * shift_eta_batch[i]\n",
    "        shift_phi_jet = np.ones((X_jet.shape[0], 1)) * shift_phi_batch[i]\n",
    "        if i == 0:\n",
    "            shift_eta = shift_eta_jet\n",
    "            shift_phi = shift_phi_jet\n",
    "        else:\n",
    "            shift_eta = np.concatenate((shift_eta, shift_eta_jet))\n",
    "            shift_phi = np.concatenate((shift_phi, shift_phi_jet))\n",
    "\n",
    "    shift = np.hstack((shift_eta, shift_phi, np.zeros((X.shape[0], 5))))\n",
    "    new_X = X + shift\n",
    "    new_X = torch.tensor(new_X).to(device)\n",
    "    bb.x = new_X\n",
    "    return bb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.392345Z",
     "start_time": "2023-07-19T22:49:04.381810Z"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_jets(batch, device):\n",
    "    bb = batch.clone()\n",
    "    bb = bb.to(\"cpu\")\n",
    "    rot_angle = np.random.rand(len(bb))*2*np.pi\n",
    "#     print(rot_angle)\n",
    "    c = np.cos(rot_angle)\n",
    "    s = np.sin(rot_angle)\n",
    "    o = np.ones_like(rot_angle)\n",
    "    z = np.zeros_like(rot_angle)\n",
    "    rot_matrix = np.array([[z, c, -s], [z, s, c], [o, z, z]])  # (3, 3, 100)\n",
    "    rot_matrix = rot_matrix.transpose(2,0,1)  # (100, 3, 3)\n",
    "\n",
    "    for i in range(len(bb)):\n",
    "        x_ = bb[i].x[:,:3]\n",
    "        new_x = np.einsum('ij,jk', bb[i].x[:,:3], rot_matrix[i])  # this is somehow (pT, eta', phi')\n",
    "        new_x[:, [0,2]] = new_x[:, [2, 0]] \n",
    "        new_x[:, [0,1]] = new_x[:, [1, 0]] # now (phi', eta', pT)\n",
    "        \n",
    "        if i == 0:\n",
    "            new_X = new_x\n",
    "        else:\n",
    "            new_X = np.concatenate((new_X, new_x), axis=0)\n",
    "\n",
    "    new_X = torch.tensor(new_X).to(device)\n",
    "    bb.x[:,:3] = new_X\n",
    "    return bb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.398801Z",
     "start_time": "2023-07-19T22:49:04.394718Z"
    }
   },
   "outputs": [],
   "source": [
    "def augmentation(batch, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Takes events of the form Batch() and applies a series of augmentations. \n",
    "    The output will have the same shape as the input\n",
    "    \"\"\"\n",
    "    batch_aug = translate_jets(batch, device=device)\n",
    "    batch_aug = rotate_jets(batch_aug, device=device)\n",
    "    return batch_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.406089Z",
     "start_time": "2023-07-19T22:49:04.401167Z"
    }
   },
   "outputs": [],
   "source": [
    "def event_augmentation(batch, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Takes events of the form Batch() and splits them into two Batch() objects representing the two views.\n",
    "\n",
    "    In this example, the two views are translated and then rotated by different amounts\n",
    "    \"\"\"\n",
    "    view1 = augmentation(batch, device=device)\n",
    "    view2 = augmentation(batch, device=device)\n",
    "\n",
    "    return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.420528Z",
     "start_time": "2023-07-19T22:49:04.409366Z"
    }
   },
   "outputs": [],
   "source": [
    "# view1, view2 = event_augmentation(batch, device)\n",
    "# print(f\"view1: {view1}\")\n",
    "# print(f\"view2: {view2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.426980Z",
     "start_time": "2023-07-19T22:49:04.423603Z"
    }
   },
   "outputs": [],
   "source": [
    "# view1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.431757Z",
     "start_time": "2023-07-19T22:49:04.429262Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(view2[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.437239Z",
     "start_time": "2023-07-19T22:49:04.434136Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(view1[0].x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the VICReg model (Transformer-based)\n",
    "https://github.com/bmdillon/JetCLR/blob/main/scripts/modules/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.465326Z",
     "start_time": "2023-07-19T22:49:04.444835Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # define and intialize the structure of the neural network\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=7,\n",
    "        model_dim=1000,\n",
    "        output_dim=1000,\n",
    "        n_heads=4,\n",
    "        dim_feedforward=1000,\n",
    "        n_layers=4,\n",
    "        learning_rate=0.00005,\n",
    "        n_head_layers=2,\n",
    "        head_norm=False,\n",
    "        dropout=0.1,\n",
    "        opt=\"adam\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # define hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_head_layers = n_head_layers\n",
    "        self.head_norm = head_norm\n",
    "        self.dropout = dropout\n",
    "        # define subnetworks\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                model_dim, n_heads, dim_feedforward=dim_feedforward, dropout=dropout\n",
    "            ),\n",
    "            n_layers,\n",
    "        )\n",
    "        # head_layers have output_dim\n",
    "        if n_head_layers == 0:\n",
    "            self.head_layers = []\n",
    "        else:\n",
    "            if head_norm:\n",
    "                self.norm_layers = nn.ModuleList([nn.LayerNorm(model_dim)])\n",
    "            self.head_layers = nn.ModuleList([nn.Linear(model_dim, output_dim)])\n",
    "            for i in range(n_head_layers - 1):\n",
    "                if head_norm:\n",
    "                    self.norm_layers.append(nn.LayerNorm(output_dim))\n",
    "                self.head_layers.append(nn.Linear(output_dim, output_dim))\n",
    "        # option to use adam or sgd\n",
    "        if opt == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        if opt == \"sgdca\" or opt == \"sgdslr\" or opt == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(\n",
    "                self.parameters(), lr=self.learning_rate, momentum=0.9\n",
    "            )\n",
    "\n",
    "    def forward(self, view1, view2, mask=None, mult_reps=False):\n",
    "        \"\"\"\n",
    "        the two views are shaped like DataBatch(x=[12329, 7], y=[256], batch=[12329], ptr=[257])\n",
    "        transformer expects (sequence_length, feature_number) so we don't need to transpose\n",
    "        \"\"\"\n",
    "        view1_representations, view2_representations = [], []\n",
    "        assert len(view1) == len(view2)\n",
    "        # produce one representation per jet\n",
    "        for i in range(len(view1)):\n",
    "            jet1 = view1[i]\n",
    "            jet2 = view2[i]\n",
    "            # make a copy\n",
    "            x_1 = jet1.x + 0.0\n",
    "            x_2 = jet2.x + 0.0\n",
    "            # cast to torch.float32 to prevent RuntimeError: mat1 and mat2 must have the same dtype\n",
    "            x_1 = x_1.to(torch.float32)\n",
    "            x_2 = x_2.to(torch.float32)\n",
    "            # embedding\n",
    "            x_1 = self.embedding(x_1)\n",
    "            x_2 = self.embedding(x_2)\n",
    "            # transformer\n",
    "            x_1 = self.transformer(x_1, mask=mask)\n",
    "            x_2 = self.transformer(x_2, mask=mask)\n",
    "            # sum over sequence dim\n",
    "            # (batch_size, model_dim)\n",
    "            x_1 = x_1.sum(0)\n",
    "            x_2 = x_2.sum(0)\n",
    "            # head\n",
    "            x_1 = self.head(x_1, mult_reps)\n",
    "            x_2 = self.head(x_2, mult_reps)\n",
    "            # append to representations list\n",
    "            view1_representations.append(x_1)\n",
    "            view2_representations.append(x_2)\n",
    "        \n",
    "        return torch.stack(view1_representations), torch.stack(view2_representations)\n",
    "\n",
    "    def head(self, x, mult_reps):\n",
    "        \"\"\"\n",
    "        calculates output of the head if it exists, i.e. if n_head_layer>0\n",
    "        returns multiple representation layers if asked for by mult_reps = True\n",
    "        input:  x shape=(batchsize, model_dim)\n",
    "                mult_reps boolean\n",
    "        output: reps shape=(batchsize, output_dim)                  for mult_reps=False\n",
    "                reps shape=(batchsize, number_of_reps, output_dim)  for mult_reps=True\n",
    "        \"\"\"\n",
    "        relu = nn.ReLU()\n",
    "        # return representations from multiple layers for evaluation\n",
    "        if mult_reps == True:\n",
    "            if self.n_head_layers > 0:\n",
    "                reps = torch.empty(x.shape[0], self.n_head_layers + 1, self.output_dim)\n",
    "                reps[:, 0] = x\n",
    "                for i, layer in enumerate(self.head_layers):\n",
    "                    # only apply layer norm on head if chosen\n",
    "                    if self.head_norm:\n",
    "                        x = self.norm_layers[i](x)\n",
    "                    x = relu(x)\n",
    "                    x = layer(x)\n",
    "                    reps[:, i + 1] = x\n",
    "                # shape (n_head_layers, output_dim)\n",
    "                return reps\n",
    "            # no head exists -> just return x in a list with dimension 1\n",
    "            else:\n",
    "                reps = x[:, None, :]\n",
    "                # shape (batchsize, 1, model_dim)\n",
    "                return reps\n",
    "        # return only last representation for contrastive loss\n",
    "        else:\n",
    "            for i, layer in enumerate(\n",
    "                self.head_layers\n",
    "            ):  # will do nothing if n_head_layers is 0\n",
    "                if self.head_norm:\n",
    "                    x = self.norm_layers[i](x)\n",
    "                x = relu(x)\n",
    "                x = layer(x)\n",
    "            # shape either (model_dim) if no head, or (output_dim) if head exists\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:04.479618Z",
     "start_time": "2023-07-19T22:49:04.467492Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VICReg(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VICReg, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.augmentation = event_augmentation\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, event):\n",
    "        # seperate the two views\n",
    "        view1, view2 = self.augmentation(event, self.device)\n",
    "\n",
    "        # encode to retrieve the representations\n",
    "        view1_representations, view2_representations = self.encoder(view1, view2)\n",
    "#         print(f\"view 1 representations: {view1_representations}\")\n",
    "#         print(f\"view 2 representations: {view2_representations}\")\n",
    "\n",
    "        # simple MLP decoder\n",
    "        view1_embeddings = self.decoder(view1_representations)\n",
    "        view2_embeddings = self.decoder(view2_representations)\n",
    "\n",
    "        # global pooling to be able to compute a loss between views of different dimensionalities\n",
    "#         view1_embeddings = global_mean_pool(view1_embeddings, view1.batch)\n",
    "#         view2_embeddings = global_mean_pool(view2_embeddings, view2.batch)\n",
    "\n",
    "        return view1_embeddings, view2_embeddings\n",
    "\n",
    "\n",
    "class ENCODER(nn.Module):\n",
    "    \"\"\"The Encoder part of VICReg which attempts to learn useful latent representations of the two views.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        width=126,\n",
    "        embedding_dim=34,\n",
    "        num_convs=2,\n",
    "    ):\n",
    "        super(ENCODER, self).__init__()\n",
    "\n",
    "        self.act = nn.ELU\n",
    "\n",
    "        # 1. same MLP for each view\n",
    "        self.nn1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, embedding_dim),\n",
    "        )\n",
    "        self.nn2 = self.nn1\n",
    "\n",
    "        # 2. same GNN for each view\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_convs):\n",
    "            self.convs.append(\n",
    "                GravNetConv(\n",
    "                    embedding_dim,\n",
    "                    embedding_dim,\n",
    "                    space_dimensions=4,\n",
    "                    propagate_dimensions=22,\n",
    "                    k=8,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, view1, view2):\n",
    "        view1_representations = self.nn1(view1.x.float())\n",
    "        view2_representations = self.nn2(view2.x.float())\n",
    "\n",
    "        # perform a series of graph convolutions\n",
    "        for num, conv in enumerate(self.convs):\n",
    "            view1_representations = conv(view1_representations, view1.batch)\n",
    "            view2_representations = conv(view2_representations, view2.batch)\n",
    "\n",
    "        return view1_representations, view2_representations\n",
    "\n",
    "\n",
    "class DECODER(nn.Module):\n",
    "    \"\"\"The Decoder part of VICReg which attempts to expand the learned latent representations\n",
    "    of the two views into a space where a loss can be computed.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=34,\n",
    "        width=126,\n",
    "        output_dim=200,\n",
    "    ):\n",
    "        super(DECODER, self).__init__()\n",
    "\n",
    "        self.act = nn.ELU\n",
    "\n",
    "        # DECODER\n",
    "        self.expander = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.expander(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:05.888837Z",
     "start_time": "2023-07-19T22:49:04.482826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VICReg(\n",
       "  (encoder): Transformer(\n",
       "    (embedding): Linear(in_features=7, out_features=1000, bias=True)\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1000, out_features=1000, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1000, out_features=1000, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1000, out_features=1000, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1000, out_features=1000, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head_layers): ModuleList(\n",
       "      (0): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "      (1): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DECODER(\n",
       "    (expander): Sequential(\n",
       "      (0): Linear(in_features=1000, out_features=126, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Linear(in_features=126, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GravNet based\n",
    "# vicreg_encoder = ENCODER(input_dim, embedding_dim=34)\n",
    "# vicreg_decoder = DECODER(embedding_dim=34, output_dim=200)\n",
    "\n",
    "# Transformer based\n",
    "vicreg_encoder = Transformer()\n",
    "vicreg_decoder = DECODER(embedding_dim=1000, output_dim=200)\n",
    "\n",
    "vicreg = VICReg(vicreg_encoder, vicreg_decoder)\n",
    "vicreg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the loss terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:05.902901Z",
     "start_time": "2023-07-19T22:49:05.892315Z"
    }
   },
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    \"\"\"Copied from VICReg paper github https://github.com/facebookresearch/vicreg/\"\"\"\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "class CovLoss(nn.Module):\n",
    "    def forward(self, view1, view2):\n",
    "        N = view1.size(0)  # batch size\n",
    "        D = view1.size(1)  # dim of representations\n",
    "\n",
    "        view1 = view1 - view1.mean(dim=0)\n",
    "        view2 = view2 - view2.mean(dim=0)\n",
    "\n",
    "        cov_view1 = (view1.T @ view1) / (N - 1)\n",
    "        cov_view2 = (view2.T @ view2) / (N - 1)\n",
    "\n",
    "        loss = off_diagonal(cov_view1).pow_(2).sum().div(D) + off_diagonal(cov_view2).pow_(2).sum().div(D)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class VarLoss(nn.Module):\n",
    "    def forward(self, view1, view2):\n",
    "        view1 = view1 - view1.mean(dim=0)\n",
    "        view2 = view2 - view2.mean(dim=0)\n",
    "\n",
    "        # variance loss\n",
    "        std_view1 = torch.sqrt(view1.var(dim=0) + 1e-10)\n",
    "        std_view2 = torch.sqrt(view2.var(dim=0) + 1e-10)\n",
    "\n",
    "        loss = torch.mean(F.relu(1 - std_view1)) / 2 + torch.mean(F.relu(1 - std_view2)) / 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T23:01:42.062381Z",
     "start_time": "2023-07-19T23:01:42.052631Z"
    }
   },
   "outputs": [],
   "source": [
    "crit_invar = nn.MSELoss()\n",
    "crit_var = VarLoss()\n",
    "crit_cov = CovLoss()\n",
    "\n",
    "max_var = 1 * batch_size\n",
    "max_cov = 1 * batch_size\n",
    "\n",
    "constraints = []\n",
    "constraints.append(mdmm.MaxConstraint(lambda: crit_var(view1_embeddings, view2_embeddings), max_var, scale=1e-4))\n",
    "constraints.append(mdmm.MaxConstraint(lambda: crit_cov(view1_embeddings, view2_embeddings), max_cov))\n",
    "# constraints.append(mdmm.MaxConstraint(lambda: crit_cov(view1_embeddings, view2_embeddings), max_cov, scale=1e4))\n",
    "\n",
    "mdmm_module = mdmm.MDMM(constraints)\n",
    "optimizer = mdmm_module.make_optimizer(vicreg.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T23:02:14.031802Z",
     "start_time": "2023-07-19T23:01:57.044664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max var: 128\n",
      "max cov: 128\n",
      "batch 0\n",
      "invariance loss: 0.01\n",
      "variance loss: 117.09\n",
      "covariance loss: 6.30\n",
      "constrained invariance loss: 0.01\n",
      "-----------\n",
      "batch 1\n",
      "invariance loss: 0.04\n",
      "variance loss: 113.15\n",
      "covariance loss: 22.19\n",
      "constrained invariance loss: 0.04\n",
      "-----------\n",
      "batch 2\n",
      "invariance loss: 0.01\n",
      "variance loss: 115.54\n",
      "covariance loss: 11.02\n",
      "constrained invariance loss: 0.01\n",
      "-----------\n",
      "batch 3\n",
      "invariance loss: 0.01\n",
      "variance loss: 116.44\n",
      "covariance loss: 7.98\n",
      "constrained invariance loss: 0.01\n",
      "-----------\n",
      "batch 4\n",
      "invariance loss: 0.01\n",
      "variance loss: 116.19\n",
      "covariance loss: 8.68\n",
      "constrained invariance loss: 0.01\n",
      "-----------\n",
      "batch 5\n",
      "invariance loss: 0.01\n",
      "variance loss: 117.19\n",
      "covariance loss: 5.94\n",
      "constrained invariance loss: 0.01\n"
     ]
    }
   ],
   "source": [
    "losses_inv, losses_var, losses_cov, losses_reg = [], [], [], []\n",
    "print(f\"max var: {max_var}\")\n",
    "print(f\"max cov: {max_cov}\")\n",
    "for i, batch in enumerate(loader):\n",
    "    print(f\"batch {i}\")\n",
    "    # run VICReg forward pass to get the embeddings\n",
    "    view1_embeddings, view2_embeddings = vicreg(batch.to(device))\n",
    "#     print(f\"view1_embeddings size: {view1_embeddings.size()}\")\n",
    "\n",
    "    # compute the invariance loss which is contrained by the other loss terms\n",
    "    loss = batch_size * crit_invar(view1_embeddings, view2_embeddings)\n",
    "    print(f\"invariance loss: {loss:.2f}\")\n",
    "    losses_inv.append(loss.detach().item())\n",
    "    loss_var = batch_size * crit_var(view1_embeddings, view2_embeddings)\n",
    "    print(f\"variance loss: {loss_var:.2f}\")\n",
    "    losses_var.append(loss_var.detach().item())\n",
    "    loss_cov = batch_size * crit_cov(view1_embeddings, view2_embeddings)\n",
    "    print(f\"covariance loss: {loss_cov:.2f}\")\n",
    "    losses_cov.append(loss_cov.detach().item())\n",
    "\n",
    "\n",
    "    mdmm_return = mdmm_module(loss.to(device))\n",
    "\n",
    "    # backprop\n",
    "    for param in vicreg.parameters():\n",
    "        param.grad = None\n",
    "    mdmm_return.value.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    print(f\"constrained invariance loss: {loss.detach():.2f}\")\n",
    "    losses_reg.append(loss.detach().item())\n",
    "\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T23:00:17.695285Z",
     "start_time": "2023-07-19T23:00:17.676859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDMMReturn(value=tensor(0.0194, device='cuda:0', grad_fn=<AddBackward0>), fn_values=[tensor(0.9093, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)], infs=[tensor(-0.0260, device='cuda:0', grad_fn=<SubBackward0>), tensor(0.0709, device='cuda:0', grad_fn=<SubBackward0>)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdmm_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T23:00:47.954787Z",
     "start_time": "2023-07-19T23:00:47.939443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.0260, device='cuda:0', grad_fn=<SubBackward0>),\n",
       " tensor(0.0709, device='cuda:0', grad_fn=<SubBackward0>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdmm_return.infs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T23:02:38.657124Z",
     "start_time": "2023-07-19T23:02:38.637830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDMMReturn(value=tensor(0.0075, device='cuda:0', grad_fn=<AddBackward0>), fn_values=[tensor(0.9155, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)], infs=[tensor(-0.0065, device='cuda:0', grad_fn=<SubBackward0>), tensor(0.0110, device='cuda:0', grad_fn=<SubBackward0>)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdmm_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:08.518626Z",
     "start_time": "2023-07-19T22:49:08.518612Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"view1_embeddings size: {view1_embeddings.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:08.519713Z",
     "start_time": "2023-07-19T22:49:08.519701Z"
    }
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:08.520967Z",
     "start_time": "2023-07-19T22:49:08.520955Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"max var: {max_var}\")\n",
    "print(f\"max cov: {max_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:08.522145Z",
     "start_time": "2023-07-19T22:49:08.522132Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_inv)\n",
    "plt.title(\"Unconstrained Invariance loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:08.525320Z",
     "start_time": "2023-07-19T22:49:08.525296Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_var)\n",
    "plt.title(\"Variance loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:08.526971Z",
     "start_time": "2023-07-19T22:49:08.526949Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_cov)\n",
    "plt.title(\"Covariance loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:49:08.529011Z",
     "start_time": "2023-07-19T22:49:08.528989Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_reg)\n",
    "plt.title(\"Constrained Invariance loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd710be4164d8116e60481776a482d6ed163c0c31d42101b2cd55e4bfc6d2c5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
