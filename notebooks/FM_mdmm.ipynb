{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mdmm` tutorial\n",
    "The purpose of this notebook is to guide you to train a model with a loss function of several parameters in a proper mathematical way. The idea comes from the paper [Constrained Differential Optimization](https://papers.nips.cc/paper/1987/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf), and the implementation from the [mdmm package Github](https://github.com/crowsonkb/mdmm)\n",
    "\n",
    "The guide is best illustrated through the **[VICReg](https://arxiv.org/abs/2105.04906)** example where your input is split into two views and you are asked to minimize three loss terms: `variance`, `invariance` and `covariance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mdmm\n",
      "  Downloading mdmm-0.1.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from mdmm) (1.13.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->mdmm) (4.5.0)\n",
      "Installing collected packages: mdmm\n",
      "Successfully installed mdmm-0.1.3\n"
     ]
    }
   ],
   "source": [
    "! pip install mdmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import mdmm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.conv import GravNetConv\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# define the global base device\n",
    "if torch.cuda.device_count():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Will use {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Will use cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a processed `.pt` clic file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clic events 100001\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"/../ssl-jet-vol-v2/toptagging/train/processed/data_0.pt\")\n",
    "print(f\"num of clic events {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single event: \n",
      " DataBatch(x=[12533, 7], y=[256], batch=[12533], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "# build a data loader\n",
    "batch_size = 256\n",
    "\n",
    "loader = DataLoader(data, batch_size, shuffle=True)\n",
    "for batch in loader:\n",
    "    print(f\"A single event: \\n {batch}\")\n",
    "    break\n",
    "\n",
    "input_dim = batch.x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_jets(batch, width=1.0, device='cpu'):\n",
    "    width = 1.0\n",
    "    batch = batch.to(\"cpu\")\n",
    "    X = batch.x.numpy()\n",
    "    mask = (X[:,2] > 0).reshape(X.shape[0],1)\n",
    "    ptp_eta = np.ptp(X[:,1], axis=-1, keepdims=True)\n",
    "    ptp_phi = np.ptp(X[:,0], axis=-1, keepdims=True)\n",
    "    low_eta  = -width*ptp_eta\n",
    "    high_eta = +width*ptp_eta\n",
    "    low_phi = np.maximum(-width*ptp_phi, -np.pi-np.min(X[:,2]).reshape(ptp_phi.shape))\n",
    "    high_phi = np.minimum(+width*ptp_phi, +np.pi-np.max(X[:,2]).reshape(ptp_phi.shape))\n",
    "    shift_eta = mask*np.random.uniform(low=low_eta, high=high_eta, size=(X.shape[0], 1))\n",
    "    shift_phi = mask*np.random.uniform(low=low_phi, high=high_phi, size=(X.shape[0], 1))\n",
    "    shift = np.hstack((shift_eta, shift_phi, np.zeros((X.shape[0], 5))))\n",
    "    X = X + shift\n",
    "    X = torch.tensor(X).to(device)\n",
    "    batch.x = X.float()\n",
    "    return batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_augmentation(batch):\n",
    "    \"\"\"\n",
    "    Takes events of the form Batch() and splits them into two Batch() objects representing the two views.\n",
    "\n",
    "    In this example, the first view is tracks and the second view is clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    view1 = translate_jets(batch)\n",
    "    view2 = translate_jets(batch)\n",
    "\n",
    "    return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view1: Batch(x=[2428, 17], ygen=[2428, 6], ygen_id=[2428], ycand=[2428, 6], ycand_id=[2428], batch=[2428])\n",
      "view2: Batch(x=[4503, 17], ygen=[4503, 6], ygen_id=[4503], ycand=[4503, 6], ycand_id=[4503], batch=[4503])\n"
     ]
    }
   ],
   "source": [
    "view1, view2 = event_augmentation(batch)\n",
    "print(f\"view1: {view1}\")\n",
    "print(f\"view2: {view2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the VICReg model (GravNet-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VICReg(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VICReg, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.augmentation = event_augmentation\n",
    "\n",
    "    def forward(self, event):\n",
    "        # seperate the two views\n",
    "        view1, view2 = self.augmentation(event)\n",
    "\n",
    "        # encode to retrieve the representations\n",
    "        view1_representations, view2_representations = self.encoder(view1, view2)\n",
    "\n",
    "        # simple MLP decoder\n",
    "        view1_embeddings = self.decoder(view1_representations)\n",
    "        view2_embeddings = self.decoder(view2_representations)\n",
    "\n",
    "        # global pooling to be able to compute a loss between views of different dimensionalities\n",
    "        view1_embeddings = global_mean_pool(view1_embeddings, view1.batch)\n",
    "        view2_embeddings = global_mean_pool(view2_embeddings, view2.batch)\n",
    "\n",
    "        return view1_embeddings, view2_embeddings\n",
    "\n",
    "\n",
    "class ENCODER(nn.Module):\n",
    "    \"\"\"The Encoder part of VICReg which attempts to learn useful latent representations of the two views.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        width=126,\n",
    "        embedding_dim=34,\n",
    "        num_convs=2,\n",
    "    ):\n",
    "        super(ENCODER, self).__init__()\n",
    "\n",
    "        self.act = nn.ELU\n",
    "\n",
    "        # 1. different MLP for each view\n",
    "        self.nn1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, embedding_dim),\n",
    "        )\n",
    "        self.nn2 = nn.Sequential(\n",
    "            nn.Linear(17, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, embedding_dim),\n",
    "        )\n",
    "\n",
    "        # 2. same GNN for each view\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_convs):\n",
    "            self.convs.append(\n",
    "                GravNetConv(\n",
    "                    embedding_dim,\n",
    "                    embedding_dim,\n",
    "                    space_dimensions=4,\n",
    "                    propagate_dimensions=22,\n",
    "                    k=8,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, view1, view2):\n",
    "        view1_representations = self.nn1(view1.x.float())\n",
    "        view2_representations = self.nn2(view2.x.float())\n",
    "\n",
    "        # perform a series of graph convolutions\n",
    "        for num, conv in enumerate(self.convs):\n",
    "            view1_representations = conv(view1_representations, view1.batch)\n",
    "            view2_representations = conv(view2_representations, view2.batch)\n",
    "\n",
    "        return view1_representations, view2_representations\n",
    "\n",
    "\n",
    "class DECODER(nn.Module):\n",
    "    \"\"\"The Decoder part of VICReg which attempts to expand the learned latent representations\n",
    "    of the two views into a space where a loss can be computed.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=34,\n",
    "        width=126,\n",
    "        output_dim=200,\n",
    "    ):\n",
    "        super(DECODER, self).__init__()\n",
    "\n",
    "        self.act = nn.ELU\n",
    "\n",
    "        # DECODER\n",
    "        self.expander = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, width),\n",
    "            self.act(),\n",
    "            nn.Linear(width, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.expander(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VICReg(\n",
       "  (encoder): ENCODER(\n",
       "    (nn1): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=126, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Linear(in_features=126, out_features=34, bias=True)\n",
       "    )\n",
       "    (nn2): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=126, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Linear(in_features=126, out_features=34, bias=True)\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): GravNetConv(34, 34, k=8)\n",
       "      (1): GravNetConv(34, 34, k=8)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DECODER(\n",
       "    (expander): Sequential(\n",
       "      (0): Linear(in_features=34, out_features=126, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): Linear(in_features=126, out_features=126, bias=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Linear(in_features=126, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vicreg_encoder = ENCODER(input_dim, embedding_dim=34)\n",
    "vicreg_decoder = DECODER(embedding_dim=34, output_dim=200)\n",
    "\n",
    "vicreg = VICReg(vicreg_encoder, vicreg_decoder)\n",
    "vicreg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the loss terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    \"\"\"Copied from VICReg paper github https://github.com/facebookresearch/vicreg/\"\"\"\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "class CovLoss(nn.Module):\n",
    "    def forward(self, view1, view2):\n",
    "        N = view1.size(0)  # batch size\n",
    "        D = view1.size(1)  # dim of representations\n",
    "\n",
    "        view1 = view1 - view1.mean(dim=0)\n",
    "        view2 = view2 - view2.mean(dim=0)\n",
    "\n",
    "        cov_view1 = (view1.T @ view1) / (N - 1)\n",
    "        cov_view2 = (view2.T @ view2) / (N - 1)\n",
    "\n",
    "        loss = off_diagonal(cov_view1).pow_(2).sum().div(D) + off_diagonal(cov_view2).pow_(2).sum().div(D)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class VarLoss(nn.Module):\n",
    "    def forward(self, view1, view2):\n",
    "        view1 = view1 - view1.mean(dim=0)\n",
    "        view2 = view2 - view2.mean(dim=0)\n",
    "\n",
    "        # variance loss\n",
    "        std_view1 = torch.sqrt(view1.var(dim=0) + 1e-10)\n",
    "        std_view2 = torch.sqrt(view2.var(dim=0) + 1e-10)\n",
    "\n",
    "        loss = torch.mean(F.relu(1 - std_view1)) / 2 + torch.mean(F.relu(1 - std_view2)) / 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_invar = nn.MSELoss()\n",
    "crit_var = VarLoss()\n",
    "crit_cov = CovLoss()\n",
    "\n",
    "max_var = 1e-5 * batch_size\n",
    "max_cov = 50 * batch_size\n",
    "\n",
    "constraints = []\n",
    "constraints.append(mdmm.MaxConstraint(lambda: crit_var(view1_embeddings, view2_embeddings), max_var))\n",
    "constraints.append(mdmm.MaxConstraint(lambda: crit_cov(view1_embeddings, view2_embeddings), max_cov, scale=1e4))\n",
    "\n",
    "mdmm_module = mdmm.MDMM(constraints)\n",
    "optimizer = mdmm_module.make_optimizer(vicreg.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrained invariance loss: 5.43\n",
      "constrained invariance loss: 3.92\n",
      "constrained invariance loss: 2.88\n",
      "constrained invariance loss: 2.14\n",
      "constrained invariance loss: 1.79\n",
      "constrained invariance loss: 1.47\n",
      "constrained invariance loss: 1.18\n",
      "constrained invariance loss: 0.98\n",
      "constrained invariance loss: 0.82\n",
      "constrained invariance loss: 0.67\n",
      "constrained invariance loss: 0.59\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(loader):\n",
    "    # run VICReg forward pass to get the embeddings\n",
    "    view1_embeddings, view2_embeddings = vicreg(batch.to(device))\n",
    "\n",
    "    # compute the invariance loss which is contrained by the other loss terms\n",
    "    loss = batch_size * crit_invar(view1_embeddings, view2_embeddings)\n",
    "    mdmm_return = mdmm_module(loss)\n",
    "\n",
    "    # backprop\n",
    "    for param in vicreg.parameters():\n",
    "        param.grad = None\n",
    "    mdmm_return.value.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    print(f\"constrained invariance loss: {loss.detach():.2f}\")\n",
    "\n",
    "    if i == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd710be4164d8116e60481776a482d6ed163c0c31d42101b2cd55e4bfc6d2c5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
