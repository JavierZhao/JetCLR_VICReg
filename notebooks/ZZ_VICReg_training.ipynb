{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc514b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.139058Z",
     "start_time": "2023-07-27T19:01:00.270622Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch import nn\n",
    "\n",
    "from src.models.transformer import Transformer\n",
    "from src.models.jet_augs import translate_jets, rotate_jets\n",
    "\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dc7488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.163131Z",
     "start_time": "2023-07-27T19:01:09.144408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# define the global base device\n",
    "if torch.cuda.device_count():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Will use {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Will use cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8f71fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.394003Z",
     "start_time": "2023-07-27T19:01:09.166002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssl-jet-vol-v2/JetCLR_VICReg/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051b659a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.402847Z",
     "start_time": "2023-07-27T19:01:09.398255Z"
    }
   },
   "outputs": [],
   "source": [
    "project_dir = \"/ssl-jet-vol-v2/JetCLR_VICReg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19547c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T17:31:39.703669Z",
     "start_time": "2023-07-27T17:31:39.688519Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f81d6ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.423388Z",
     "start_time": "2023-07-27T19:01:09.408495Z"
    }
   },
   "outputs": [],
   "source": [
    "class VICReg(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.num_features = int(\n",
    "            args.mlp.split(\"-\")[-1]\n",
    "        )  # size of the last layer of the MLP projector\n",
    "        self.x_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.x_inputs),\n",
    "            nn.Linear(args.x_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.y_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.y_inputs),\n",
    "            nn.Linear(args.y_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.augmentation = args.augmentation\n",
    "        self.x_backbone = args.x_backbone\n",
    "        self.y_backbone = args.y_backbone\n",
    "        self.N_x = self.x_backbone.input_dim\n",
    "        self.N_y = self.y_backbone.input_dim\n",
    "        self.embedding = args.Do\n",
    "        self.return_embedding = args.return_embedding\n",
    "        self.return_representation = args.return_representation\n",
    "        self.x_projector = Projector(args.mlp, self.embedding)\n",
    "        self.y_projector = (\n",
    "            self.x_projector if args.shared else copy.deepcopy(self.x_projector)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x -> x_aug -> x_xform -> x_rep -> x_emb\n",
    "        y -> y_aug -> y_xform -> y_rep -> y_emb\n",
    "        _aug: augmented\n",
    "        _xform: transformed\n",
    "        _rep: backbone representation\n",
    "        _emb: projected embedding\n",
    "        \"\"\"\n",
    "        # x: [N_x, x_inputs]\n",
    "        # y: [N_y, y_inputs]\n",
    "        x_aug = self.augmentation(self.args, x, self.args.device)\n",
    "        y_aug = self.augmentation(self.args, y, self.args.device)\n",
    "        \n",
    "        x_xform = x_aug\n",
    "        y_xform = y_aug\n",
    "        x_xform.x = self.x_transform.to(torch.double)(x_aug.x.double())  # [N_x, transform_inputs]?\n",
    "        y_xform.x = self.y_transform.to(torch.double)(y_aug.x.double())  # [N_y, transform_inputs]?\n",
    "        \n",
    "        x_rep = self.x_backbone(x_aug)  # [batch_size, output_dim]\n",
    "        y_rep = self.y_backbone(y_aug)  # [batch_size, output_dim]\n",
    "        if self.return_representation:\n",
    "            return x_rep, y_rep\n",
    "\n",
    "        x_emb = self.x_projector(x_rep)  # [batch_size, embedding_size]\n",
    "        y_emb = self.y_projector(y_rep)  # [batch_size, embedding_size]\n",
    "        if self.return_embedding:\n",
    "            return x_emb, y_emb\n",
    "        x = x_emb\n",
    "        y = y_emb\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "\n",
    "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
    "        cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "            self.num_features\n",
    "        ) + off_diagonal(cov_y).pow_(2).sum().div(self.num_features)\n",
    "\n",
    "        loss = (\n",
    "            self.args.sim_coeff * repr_loss\n",
    "            + self.args.std_coeff * std_loss\n",
    "            + self.args.cov_coeff * cov_loss\n",
    "        )\n",
    "        if args.return_all_losses:\n",
    "            return loss, repr_loss, std_loss, cov_loss\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ea8478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.436241Z",
     "start_time": "2023-07-27T19:01:09.425636Z"
    }
   },
   "outputs": [],
   "source": [
    "def Projector(mlp, embedding):\n",
    "    mlp_spec = f\"{embedding}-{mlp}\"\n",
    "    layers = []\n",
    "    f = list(map(int, mlp_spec.split(\"-\")))\n",
    "    for i in range(len(f) - 2):\n",
    "        layers.append(nn.Linear(f[i], f[i + 1]))\n",
    "        layers.append(nn.BatchNorm1d(f[i + 1]))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(f[-2], f[-1], bias=False))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "def get_backbones(args):\n",
    "    x_backbone = Transformer(input_dim=args.transform_inputs)\n",
    "    y_backbone = x_backbone if args.shared else copy.deepcopy(x_backbone)\n",
    "    return x_backbone, y_backbone\n",
    "\n",
    "\n",
    "def augmentation(args, batch, device):\n",
    "    \"\"\"\n",
    "    batch: DataBatch(x=[12329, 7], y=[256], batch=[12329], ptr=[257])\n",
    "    \"\"\"\n",
    "    if args.do_translation:\n",
    "        batch = translate_jets(batch, device, width=1.0)\n",
    "    if args.do_rotation:\n",
    "        batch = rotate_jets(batch, device)\n",
    "    return batch.to(device)\n",
    "\n",
    "# load the datafiles\n",
    "def load_data(dataset_path, flag, n_files=-1):\n",
    "    data_files = glob.glob(f\"{dataset_path}/{flag}/processed/*\")\n",
    "\n",
    "    data = []\n",
    "    for i, file in enumerate(data_files):\n",
    "        data += torch.load(f\"{dataset_path}/{flag}/processed/data_{i}.pt\")\n",
    "        print(f\"--- loaded file {i} from `{flag}` directory\")\n",
    "        if n_files != -1 and i == n_files - 1:\n",
    "            break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e26e304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.442807Z",
     "start_time": "2023-07-27T19:01:09.438358Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11233735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:01:09.452786Z",
     "start_time": "2023-07-27T19:01:09.445371Z"
    }
   },
   "outputs": [],
   "source": [
    "args.epoch = 100\n",
    "args.batch_size = 126\n",
    "args.outdir = f\"{project_dir}/models/\"\n",
    "args.label = \"initial\"\n",
    "args.dataset_path = \"/ssl-jet-vol-v2/toptagging\"\n",
    "args.num_train_files = 3\n",
    "args.num_val_files = 1\n",
    "args.shared = False\n",
    "args.device = device\n",
    "args.mlp = \"256-256-256\"\n",
    "args.transform_inputs = 32\n",
    "args.Do = 1000\n",
    "args.hidden = 128\n",
    "args.sim_coeff = 25.0\n",
    "args.std_coeff = 25.0\n",
    "args.cov_coeff = 1.0\n",
    "args.return_embedding = False\n",
    "args.return_representation = False\n",
    "args.do_translation = True\n",
    "args.do_rotation = True\n",
    "args.return_all_losses = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e95901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:04:41.120208Z",
     "start_time": "2023-07-27T19:01:18.893354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- loaded file 0 from `train` directory\n",
      "--- loaded file 1 from `train` directory\n",
      "--- loaded file 2 from `train` directory\n"
     ]
    }
   ],
   "source": [
    "n_epochs = args.epoch\n",
    "batch_size = args.batch_size\n",
    "outdir = args.outdir\n",
    "label = args.label\n",
    "args.augmentation = augmentation\n",
    "args.x_inputs = 7\n",
    "args.y_inputs = 7\n",
    "\n",
    "model_loc = f\"{outdir}/trained_models/\"\n",
    "model_perf_loc = f\"{outdir}/model_performances/\"\n",
    "model_dict_loc = f\"{outdir}/model_dicts/\"\n",
    "os.system(\n",
    "    f\"mkdir -p {model_loc} {model_perf_loc} {model_dict_loc}\"\n",
    ")  # -p: create parent dirs if needed, exist_ok\n",
    "\n",
    "# prepare data\n",
    "data_train = load_data(args.dataset_path, \"train\", n_files=args.num_train_files)\n",
    "# data_valid = load_data(args.dataset_path, \"val\", n_files=args.num_val_files)\n",
    "\n",
    "n_train = sum([len(d) for d in data_train])\n",
    "# n_val = sum([len(d) for d in data_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59fa1511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:04:48.700459Z",
     "start_time": "2023-07-27T19:04:48.685403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54dfc927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T19:05:16.753686Z",
     "start_time": "2023-07-27T19:05:16.746072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[23, 7], y=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccf4ac8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:39:27.723792Z",
     "start_time": "2023-07-27T18:38:58.063147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training loss: 21.6019:   0%|                                                                                                                                       | 0/1587 [03:37<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "Training loss: 21.7369:   0%|                                                                                                                                       | 0/1587 [00:03<?, ?it/s]\u001b[A\u001b[A\n",
      "1it [00:03,  3.08s/it]\u001b[A\n",
      "\n",
      "Training loss: 21.6547:   0%|                                                                                                                                       | 0/1587 [00:06<?, ?it/s]\u001b[A\u001b[A\n",
      "2it [00:06,  3.01s/it]\u001b[A\n",
      "\n",
      "Training loss: 21.2681:   0%|                                                                                                                                       | 0/1587 [00:08<?, ?it/s]\u001b[A\u001b[A\n",
      "3it [00:08,  2.98s/it]\u001b[A\n",
      "\n",
      "Training loss: 20.9294:   0%|                                                                                                                                       | 0/1587 [00:11<?, ?it/s]\u001b[A\u001b[A\n",
      "4it [00:11,  2.93s/it]\u001b[A\n",
      "\n",
      "Training loss: 20.8416:   0%|                                                                                                                                       | 0/1587 [00:14<?, ?it/s]\u001b[A\u001b[A\n",
      "5it [00:14,  2.92s/it]\u001b[A\n",
      "\n",
      "Training loss: 20.5253:   0%|                                                                                                                                       | 0/1587 [00:17<?, ?it/s]\u001b[A\u001b[A\n",
      "6it [00:17,  2.93s/it]\u001b[A\n",
      "\n",
      "Training loss: 20.3668:   0%|                                                                                                                                       | 0/1587 [00:20<?, ?it/s]\u001b[A\u001b[A\n",
      "7it [00:20,  2.91s/it]\u001b[A\n",
      "\n",
      "Training loss: 20.1395:   0%|                                                                                                                                       | 0/1587 [00:23<?, ?it/s]\u001b[A\u001b[A\n",
      "8it [00:23,  2.91s/it]\u001b[A\n",
      "\n",
      "Training loss: 20.0604:   0%|                                                                                                                                       | 0/1587 [00:26<?, ?it/s]\u001b[A\u001b[A\n",
      "9it [00:29,  3.24s/it]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch, copy\u001b[38;5;241m.\u001b[39mdeepcopy(batch))\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "args.x_backbone, args.y_backbone = get_backbones(args)\n",
    "model = VICReg(args).to(args.device)\n",
    "\n",
    "train_its = int(n_train / batch_size)\n",
    "val_its = int(n_val / batch_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_val_epochs = []  # loss recorded for each epoch\n",
    "repr_loss_val_epochs = []  # invariance loss recorded for each epoch\n",
    "std_loss_val_epochs = []  # variance loss recorded for each epoch\n",
    "cov_loss_val_epochs = []  # covariance loss recorded for each epoch\n",
    "loss_val_batches = []  # loss recorded for each batch\n",
    "loss_train_epochs = []  # loss recorded for each epoch\n",
    "repr_loss_train_epochs = []  # invariance loss recorded for each epoch\n",
    "std_loss_train_epochs = []  # variance loss recorded for each epoch\n",
    "cov_loss_train_epochs = []  # covariance loss recorded for each epoch\n",
    "loss_train_batches = []  # loss recorded for each batch\n",
    "l_val_best = 999999\n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(f\"Epoch {m}\\n\")\n",
    "    loss_train_epoch = []  # loss recorded for each batch in this epoch\n",
    "    repr_loss_train_epoch, std_loss_train_epoch, cov_loss_train_epoch = [], [], []\n",
    "    # invariance, variance, covariance loss recorded for each batch in this epoch\n",
    "    loss_val_epoch = []  # loss recorded for each batch in this epoch\n",
    "    repr_loss_val_epoch, std_loss_val_epoch, cov_loss_val_epoch = [], [], []\n",
    "    # invariance, variance, covariance loss recorded for each batch in this epoch\n",
    "        \n",
    "    train_loader = DataLoader(data_train, batch_size)\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(train_loader, total=train_its)\n",
    "    for _, batch in tqdm.tqdm(enumerate(train_loader)):\n",
    "        batch = batch.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        if args.return_all_losses:\n",
    "            loss, repr_loss, std_loss, cov_loss = model.forward(batch, copy.deepcopy(batch))\n",
    "            repr_loss_train_epoch.append(repr_loss.detach().cpu().item())\n",
    "            std_loss_train_epoch.append(std_loss.detach().cpu().item())\n",
    "            cov_loss_train_epoch.append(cov_loss.detach().cpu().item())\n",
    "        else:\n",
    "            loss = model.forward(batch, copy.deepcopy(batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.detach().cpu().item()\n",
    "        loss_train_batches.append(loss)\n",
    "        loss_train_epoch.append(loss)\n",
    "        pbar.set_description(f\"Training loss: {loss:.4f}\")\n",
    "    model.eval()\n",
    "    valid_loader = DataLoader(data_valid, batch_size)\n",
    "    pbar = tqdm.tqdm(valid_loader, total=val_its)\n",
    "    for _, batch in tqdm.tqdm(enumerate(valid_loader)):\n",
    "        batch = batch.to(args.device)\n",
    "        if args.return_all_losses:\n",
    "            loss, repr_loss, std_loss, cov_loss = model.forward(batch, copy.deepcopy(batch))\n",
    "            repr_loss_val_epoch.append(repr_loss.detach().cpu().item())\n",
    "            std_loss_val_epoch.append(std_loss.detach().cpu().item())\n",
    "            cov_loss_val_epoch.append(cov_loss.detach().cpu().item())\n",
    "            loss = loss.detach().cpu().item()\n",
    "        else:\n",
    "            loss = model.forward(batch, batch.deepcopy()).cpu().item()\n",
    "        loss_val_batches.append(loss)\n",
    "        loss_val_epoch.append(loss)\n",
    "        pbar.set_description(f\"Validation loss: {loss:.4f}\")\n",
    "    l_val = np.mean(np.array(loss_val_epoch))\n",
    "    l_train = np.mean(np.array(loss_train_epoch))\n",
    "    loss_val_epochs.append(l_val)\n",
    "    loss_train_epochs.append(l_train)\n",
    "\n",
    "    if args.return_all_losses:\n",
    "        repr_l_val = np.mean(np.array(repr_loss_val_epoch))\n",
    "        repr_l_train = np.mean(np.array(repr_loss_train_epoch))\n",
    "        std_l_val = np.mean(np.array(std_loss_val_epoch))\n",
    "        std_l_train = np.mean(np.array(std_loss_train_epoch))\n",
    "        cov_l_val = np.mean(np.array(cov_loss_val_epoch))\n",
    "        cov_l_train = np.mean(np.array(cov_loss_train_epoch))\n",
    "\n",
    "        repr_loss_val_epochs.append(repr_l_val)\n",
    "        std_loss_val_epochs.append(std_l_val)\n",
    "        cov_loss_val_epochs.append(cov_l_val)\n",
    "\n",
    "        repr_loss_train_epochs.append(repr_l_train)\n",
    "        std_loss_train_epochs.append(std_l_train)\n",
    "        cov_loss_train_epochs.append(cov_l_train)\n",
    "    # save the model\n",
    "    if l_val < l_val_best:\n",
    "        print(\"New best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(model.state_dict(), f\"{model_loc}/vicreg_{label}_best.pth\")\n",
    "    torch.save(model.state_dict(), f\"{model_loc}/vicreg_{label}_last.pth\")\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_train_epochs.npy\",\n",
    "    np.array(loss_train_epochs),\n",
    ")\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_train_batches.npy\",\n",
    "    np.array(loss_train_batches),\n",
    ")\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_val_epochs.npy\",\n",
    "    np.array(loss_val_epochs),\n",
    ")\n",
    "np.save(\n",
    "    f\"{model_perf_loc}/vicreg_{label}_loss_val_batches.npy\",\n",
    "    np.array(loss_val_batches),\n",
    ")\n",
    "if args.return_all_losses:\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_repr_loss_train_epochs.npy\",\n",
    "        np.array(repr_loss_train_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_std_loss_train_epochs.npy\",\n",
    "        np.array(std_loss_train_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_cov_loss_train_epochs.npy\",\n",
    "        np.array(cov_loss_train_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_repr_loss_val_epochs.npy\",\n",
    "        np.array(repr_loss_val_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_std_loss_val_epochs.npy\",\n",
    "        np.array(std_loss_val_epochs),\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{model_perf_loc}/vicreg_{label}_cov_loss_val_epochs.npy\",\n",
    "        np.array(cov_loss_val_epochs),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf621f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
