{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0060331a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.data.h5data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mh5data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m H5Data\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphNetEmbedding\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.data.h5data'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch import nn\n",
    "\n",
    "from src.data.h5data import H5Data\n",
    "from src.models.models import GraphNetEmbedding\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    import setGPU  # noqa: F401\n",
    "\n",
    "project_dir = Path(__file__).resolve().parents[2]\n",
    "\n",
    "train_path = \"/ssl-jet-vol/hbb_interaction_network/data/processed/train/\"\n",
    "definitions = f\"{project_dir}/src/data/definitions.yml\"\n",
    "with open(definitions) as yaml_file:\n",
    "    defn = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "N = defn[\"nobj_2\"]  # number of charged particles\n",
    "N_sv = defn[\"nobj_3\"]  # number of SVs\n",
    "n_targets = len(defn[\"reduced_labels\"])  # number of classes\n",
    "params = defn[\"features_2\"]\n",
    "params_sv = defn[\"features_3\"]\n",
    "\n",
    "\n",
    "class VICReg(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.num_features = int(args.mlp.split(\"-\")[-1])\n",
    "        self.x_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.x_inputs),\n",
    "            nn.Linear(args.x_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.y_transform = nn.Sequential(\n",
    "            nn.BatchNorm1d(args.y_inputs),\n",
    "            nn.Linear(args.y_inputs, args.transform_inputs),\n",
    "            nn.BatchNorm1d(args.transform_inputs),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.x_backbone = args.x_backbone\n",
    "        self.y_backbone = args.y_backbone\n",
    "        self.N_x = self.x_backbone.N\n",
    "        self.N_y = self.y_backbone.N\n",
    "        self.embedding = args.Do\n",
    "        self.projector = Projector(args, self.embedding)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x: [batch, x_inputs, N_x]\n",
    "        # y: (batch, y_inputs, N_y]\n",
    "        x = x.transpose(-1, -2).contiguous()  # [batch, N_x, x_inputs]\n",
    "        y = y.transpose(-1, -2).contiguous()  # [batch, N_y, y_inputs]\n",
    "        x = self.x_transform(x.view(-1, self.args.x_inputs)).view(\n",
    "            -1, self.N_x, self.args.transform_inputs\n",
    "        )  # [batch, N_x, transform_inputs]\n",
    "        y = self.y_transform(y.view(-1, self.args.y_inputs)).view(\n",
    "            -1, self.N_y, self.args.transform_inputs\n",
    "        )  # [batch, N_y, transform_inputs]\n",
    "        x = x.transpose(-1, -2).contiguous()  # [batch, x_inputs, N_x]\n",
    "        y = y.transpose(-1, -2).contiguous()  # [batch, y_inputs, N_y]\n",
    "        x = self.projector(self.x_backbone(x))\n",
    "        y = self.projector(self.y_backbone(y))\n",
    "\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "\n",
    "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
    "        cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(self.num_features) + off_diagonal(cov_y).pow_(2).sum().div(\n",
    "            self.num_features\n",
    "        )\n",
    "\n",
    "        loss = self.args.sim_coeff * repr_loss + self.args.std_coeff * std_loss + self.args.cov_coeff * cov_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "def Projector(args, embedding):\n",
    "    mlp_spec = f\"{embedding}-{args.mlp}\"\n",
    "    layers = []\n",
    "    f = list(map(int, mlp_spec.split(\"-\")))\n",
    "    for i in range(len(f) - 2):\n",
    "        layers.append(nn.Linear(f[i], f[i + 1]))\n",
    "        layers.append(nn.BatchNorm1d(f[i + 1]))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(f[-2], f[-1], bias=False))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    files = glob.glob(os.path.join(train_path, \"newdata_*.h5\"))\n",
    "    # take first 10% of files for validation\n",
    "    # n_files_val should be 5 for full dataset\n",
    "    n_files_val = max(1, int(0.1 * len(files)))\n",
    "    files_val = files[:n_files_val]\n",
    "    files_train = files[n_files_val:]\n",
    "\n",
    "    n_epochs = args.epoch\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    data_train = H5Data(\n",
    "        batch_size=batch_size,\n",
    "        cache=None,\n",
    "        preloading=0,\n",
    "        features_name=\"training_subgroup\",\n",
    "        labels_name=\"target_subgroup\",\n",
    "        spectators_name=\"spectator_subgroup\",\n",
    "    )\n",
    "    data_train.set_file_names(files_train)\n",
    "    data_val = H5Data(\n",
    "        batch_size=batch_size,\n",
    "        cache=None,\n",
    "        preloading=0,\n",
    "        features_name=\"training_subgroup\",\n",
    "        labels_name=\"target_subgroup\",\n",
    "        spectators_name=\"spectator_subgroup\",\n",
    "    )\n",
    "    data_val.set_file_names(files_val)\n",
    "\n",
    "    n_train = data_train.count_data()\n",
    "    n_val = data_val.count_data()\n",
    "\n",
    "    args.x_inputs = len(params)\n",
    "    args.y_inputs = len(params_sv)\n",
    "\n",
    "    fr = nn.Sequential(\n",
    "        nn.Linear(2 * args.transform_inputs, args.hidden),\n",
    "        nn.BatchNorm1d(args.hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden, args.hidden),\n",
    "        nn.BatchNorm1d(args.hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden, args.De),\n",
    "        nn.BatchNorm1d(args.De),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    fo = nn.Sequential(\n",
    "        nn.Linear(args.transform_inputs + args.De, args.hidden),\n",
    "        nn.BatchNorm1d(args.hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden, args.hidden),\n",
    "        nn.BatchNorm1d(args.hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden, args.Do),\n",
    "        nn.BatchNorm1d(args.Do),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    args.x_backbone = GraphNetEmbedding(\n",
    "        n_constituents=N,\n",
    "        n_features=args.transform_inputs,\n",
    "        fr=fr,\n",
    "        fo=fo,\n",
    "        De=args.De,\n",
    "        Do=args.Do,\n",
    "        device=args.device,\n",
    "    )\n",
    "    args.y_backbone = GraphNetEmbedding(\n",
    "        n_constituents=N_sv,\n",
    "        n_features=args.transform_inputs,\n",
    "        fr=fr,\n",
    "        fo=fo,\n",
    "        De=args.De,\n",
    "        Do=args.Do,\n",
    "        device=args.device,\n",
    "    )\n",
    "\n",
    "    model = VICReg(args).to(args.device)\n",
    "\n",
    "    train_its = int(n_train / batch_size)\n",
    "    val_its = int(n_val / batch_size)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_val = []\n",
    "    loss_train = []\n",
    "    l_val_best = 999999\n",
    "    for m in range(n_epochs):\n",
    "        print(f\"Epoch {m}\\n\")\n",
    "        loss_val_epoch = []\n",
    "        train_iterator = data_train.generate_data()\n",
    "        model.train()\n",
    "        pbar = tqdm.tqdm(train_iterator, total=train_its)\n",
    "        for element in pbar:\n",
    "            (sub_X, _, _) = element\n",
    "            x = torch.tensor(sub_X[2], dtype=torch.float, device=args.device)\n",
    "            y = torch.tensor(sub_X[3], dtype=torch.float, device=args.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train.append(loss.item())\n",
    "            pbar.set_description(f\"Training loss: {loss.item():.4f}\")\n",
    "        model.eval()\n",
    "        val_iterator = data_val.generate_data()\n",
    "        pbar = tqdm.tqdm(val_iterator, total=val_its)\n",
    "        for element in pbar:\n",
    "            (sub_X, _, _) = element\n",
    "            x = torch.tensor(sub_X[2], dtype=torch.float, device=args.device)\n",
    "            y = torch.tensor(sub_X[3], dtype=torch.float, device=args.device)\n",
    "            loss = model.forward(x, y)\n",
    "            loss_val.append(loss.item())\n",
    "            loss_val_epoch.append(loss.item())\n",
    "            pbar.set_description(f\"Validation loss: {loss.item():.4f}\")\n",
    "\n",
    "        l_val = np.mean(np.array(loss_val_epoch))\n",
    "        if l_val < l_val_best:\n",
    "            print(\"New best model\")\n",
    "            l_val_best = l_val\n",
    "            torch.save(model.state_dict(), \"vicreg_best.pth\")\n",
    "        torch.save(model.state_dict(), \"vicreg_last.pth\")\n",
    "        np.save(\n",
    "            \"vicreg_loss_train.npy\",\n",
    "            np.array(loss_train),\n",
    "        )\n",
    "        np.save(\n",
    "            \"vicreg_loss_val.npy\",\n",
    "            np.array(loss_val),\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"This is executed when run from the command line\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--outdir\",\n",
    "        type=str,\n",
    "        action=\"store\",\n",
    "        dest=\"outdir\",\n",
    "        default=f\"{project_dir}/models/\",\n",
    "        help=\"Output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--transform-inputs\",\n",
    "        type=int,\n",
    "        action=\"store\",\n",
    "        dest=\"transform_inputs\",\n",
    "        default=64,\n",
    "        help=\"transform_inputs\",\n",
    "    )\n",
    "    parser.add_argument(\"--De\", type=int, action=\"store\", dest=\"De\", default=20, help=\"De\")\n",
    "    parser.add_argument(\"--Do\", type=int, action=\"store\", dest=\"Do\", default=24, help=\"Do\")\n",
    "    parser.add_argument(\"--hidden\", type=int, action=\"store\", dest=\"hidden\", default=60, help=\"hidden\")\n",
    "    parser.add_argument(\"--epoch\", type=int, action=\"store\", dest=\"epoch\", default=100, help=\"Epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--label\",\n",
    "        type=str,\n",
    "        action=\"store\",\n",
    "        dest=\"label\",\n",
    "        default=\"\",\n",
    "        help=\"a label for the model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        action=\"store\",\n",
    "        dest=\"batch_size\",\n",
    "        default=1024,\n",
    "        help=\"batch_size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        action=\"store\",\n",
    "        dest=\"device\",\n",
    "        default=\"cuda\",\n",
    "        help=\"device to train gnn; follow pytorch convention\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mlp\",\n",
    "        default=\"512-512-512\",\n",
    "        help=\"Size and number of layers of the MLP expander head\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sim-coeff\",\n",
    "        type=float,\n",
    "        default=25.0,\n",
    "        help=\"Invariance regularization loss coefficient\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--std-coeff\",\n",
    "        type=float,\n",
    "        default=25.0,\n",
    "        help=\"Variance regularization loss coefficient\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cov-coeff\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Covariance regularization loss coefficient\",\n",
    "    ) \n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0c98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
